{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import folium\n",
    "from branca.colormap import LinearColormap, StepColormap\n",
    "from folium.features import GeoJsonTooltip, CustomIcon, DivIcon\n",
    "from folium.plugins import TimestampedGeoJson, HeatMap\n",
    "from branca.element import Element, Template, MacroElement\n",
    "from shapely import wkt\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.affinity import rotate\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely.geometry import Point\n",
    "import math\n",
    "from bokeh.resources import INLINE\n",
    "from IPython.display import HTML\n",
    "from branca.colormap import linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00132d0-d0bc-4b58-97db-8a56c2bca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set panel extension\n",
    "pn.extension('plotly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55623982-2f07-47d0-9273-f089e6109baa",
   "metadata": {},
   "source": [
    "# Dashboard Idea\n",
    "\n",
    "The overall idea of this dashboard was to visualise the pollution rates in comparison to the current weather and \n",
    "assess if any trend can be identified. As well as this, I wanted a platform where this data could be looked at somewhat casually, as one may \n",
    "view daily weather on their way to work. This is so that the map can be used by both data scientists, and the average person, giving it various usages to different parties. The idea  to use hourly data rather than daily or weekly data (that may be more common for\n",
    "maps of similar nature) was influenced by the premise previously described, as hourly data would be most valuable to the casual user, and \n",
    "can still be beneficial for data science, especially considering that many weather/pollution analyses make use of data from a larger time period.\n",
    "Current popular studies such by Yansui et al [1] and Dastoorpoor et al [2], do not focus on hourly data, but rather daily and yearly, neither of \n",
    "which encapsulate what i want to show.\n",
    "\n",
    "Fig 1 is an example of a pollution / weather map, with data sourced from the gov.uk website. Rather than being hourly data, it is bi-yearly data for all of the uk. By comparing this map to the final map, which displays hourly data, it is apparent that yearly could only be used for data analysis means, which defeats the purpose of what I would like to create, however it does show some interesting trends when explored, I will leave the exploration up to the reader.\n",
    "\n",
    "\n",
    "Despite the use of hourly data, in theory we could have years worth of data in the dataset, which could tell even more of a story, however this\n",
    "amount of data was not available and is largely unfeasible\n",
    "\n",
    "I think it is an important thing to map as pollutant rates in contrast to weather are not discussed much, and improving accessibility to such data can be important as it raises awareness on pollution rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f0578-2cc6-45bc-97fe-f60fb8e3839b",
   "metadata": {},
   "source": [
    "# data\n",
    "we are using data from various sources, to gather weather data, we fetched data from a live API that returns : station coords, relative_humidity, air temperature,\n",
    "precipitation,wind speed and wind direction for the time of request. This data is gathered by 3 different station based around singapore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af71f98-9eb1-4d25-a8c7-977937e266a2",
   "metadata": {},
   "source": [
    "# Design choices in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61168-eeb0-4a66-88ea-048df1780a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load data\n",
    "data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n",
    "\n",
    "def uk_weather_map(data,uk_shapefile,year, weather_param):\n",
    "\n",
    "    # filter data for the specific year\n",
    "    specific_year_data = data[data['Year'] == year]\n",
    "\n",
    "    # fetch the value for the weather parameter for the given year\n",
    "    if specific_year_data.empty:\n",
    "        raise ValueError(f\"No data available for the specified year for {pollution_param}\")\n",
    "    \n",
    "    value = specific_year_data[weather_param].iloc[0]\n",
    "\n",
    "    # Define range for the weather parameter\n",
    "    global_min = data[weather_param].min()\n",
    "    global_max = data[weather_param].max()\n",
    "\n",
    "    # Define colourmap\n",
    "    colormap = linear.YlOrRd_09.scale(global_min, global_max)\n",
    "    colormap.caption = f'avg {weather_param.capitalize()} Level for {year}'\n",
    "\n",
    "    # Initialise map centered around the UK\n",
    "    m = folium.Map(location=[54.7023545, -3.2765753], zoom_start=6)\n",
    "\n",
    "    # Style function to color the entire shapefile based on the value\n",
    "    def style_function(feature):\n",
    "        return {\n",
    "            'fillColor': colormap(value),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.5\n",
    "        }\n",
    "\n",
    "    # Add the GeoJSON layer for the UK shapefile\n",
    "    folium.GeoJson(\n",
    "        uk_shapefile,\n",
    "        style_function=style_function,\n",
    "        tooltip=f' avg {weather_param.capitalize()}: {value}'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "\n",
    "    return m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc28f6-7f36-46ef-a065-a491249fd1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uk_pollution_map(data, shapefile, year, pollution_param):\n",
    "    # Filter data for the specific year\n",
    "    specific_year_data = data[data['Year'] == year]\n",
    "\n",
    "    # Fetch the value for the weather parameter for the given year\n",
    "    if specific_year_data.empty:\n",
    "        raise ValueError(f\"No data available for the specified year for {pollution_param}\")\n",
    "    \n",
    "    value = specific_year_data[pollution_param].iloc[0]\n",
    "\n",
    "    # Define the global range for the weather parameter\n",
    "    global_min = data[pollution_param].min()\n",
    "    global_max = data[pollution_param].max()\n",
    "\n",
    "    # Create a colormap\n",
    "    colormap = linear.YlOrRd_09.scale(global_min, global_max)\n",
    "    colormap.caption = f'Avg {pollution_param.capitalize()} Level for {year}'\n",
    "\n",
    "    # Initialise the map centered on the UK\n",
    "    m = folium.Map(location=[54.7023545, -3.2765753], zoom_start=6)\n",
    "\n",
    "    # Style function to color the entire shapefile based on the value\n",
    "    def style_function(feature):\n",
    "        return {\n",
    "            'fillColor': colormap(value),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.5\n",
    "        }\n",
    "\n",
    "    # Add the GeoJSON layer for the UK shapefile\n",
    "    folium.GeoJson(\n",
    "        shapefile,\n",
    "        style_function=style_function,\n",
    "        tooltip=f'Avg {pollution_param.capitalize()}: {value}'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9fd59-3011-4aa3-b4ae-2d6f0926b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "pollution_data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc8462-985b-475a-b959-cf5944f1b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp slider\n",
    "\n",
    "uk_year_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Year',\n",
    "    options=[str(year) for year in [2015, 2017, 2019, 2021]],  #  list of years\n",
    "    value='2015',  # Start on 2015\n",
    "    width=1000, \n",
    "    height=30\n",
    ")\n",
    "\n",
    "# Select box widgets\n",
    "uk_weather_selector = pn.widgets.Select(name='Choose weather:', options=['temperature','rainfall'], width=200)\n",
    "uk_pollutant_selector = pn.widgets.Select(name='Choose pollutant:', options=['PM10', 'PM25'], width=200)\n",
    "# Spacer to for widget design\n",
    "row_spacer = pn.Spacer(height=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35def31-fa8f-491d-9acb-ab4babcd1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update function for weather map\n",
    "@pn.depends(year=uk_year_slider.param.value, weather_param=uk_weather_selector.param.value)\n",
    "def update_uk_weather_map(year, weather_param):\n",
    "    map_object = uk_weather_map(data, uk_shapefile, int(year), weather_param)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "# Update function for pollution map\n",
    "@pn.depends(year=uk_year_slider.param.value, pollution_param=uk_pollutant_selector.param.value)\n",
    "def update_uk_pollution_map(year, pollution_param):\n",
    "    map_object = uk_pollution_map(data, uk_shapefile, int(year), pollution_param)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8a526-cd9c-430d-a099-73235a812046",
   "metadata": {},
   "source": [
    "# Fig 1\n",
    "## Please bear with Fig 1, it does update, it is just slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827bbed-6c0e-409a-970c-a7f012d6a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into layout\n",
    "layout = pn.Column(\n",
    "    pn.Row(uk_weather_selector,uk_pollutant_selector),\n",
    "    pn.Row(update_uk_weather_map, update_uk_pollution_map),\n",
    "    pn.Row(row_spacer),\n",
    "    pn.Row(uk_year_slider)\n",
    ")\n",
    "\n",
    "# Run the layout dashbaord\n",
    "layout.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbb87d-cecd-4834-a46c-50768a8f313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save dashboard as html\n",
    "layout.save('layout.html', embed=True, resources=INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871558a2-ca08-4e55-81d0-c10cd1d42cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print html map\n",
    "with open('layout.html', 'r',encoding = 'utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ce2ea-05b6-4e8b-ab34-0ea5b9bbe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# URL for the API endpoints\n",
    "BASE_URL = \"https://api.data.gov.sg/v1/environment/\"\n",
    "\n",
    "# API endpoints\n",
    "endpoints = {\n",
    "    \"relative_humidity\": \"relative-humidity\",\n",
    "    \"air_temperature\": \"air-temperature\",\n",
    "    \"precipitation\": \"rainfall\",\n",
    "    \"wind_speed\": \"wind-speed\",\n",
    "    \"wind_direction\": \"wind-direction\"\n",
    "}\n",
    "\n",
    "# Fetch data from each endpoint\n",
    "data = {}\n",
    "for key, endpoint in endpoints.items():\n",
    "    response = requests.get(f\"{BASE_URL}{endpoint}\")\n",
    "    if response.status_code == 200:\n",
    "        readings = response.json()['items'][0]['readings']\n",
    "        for reading in readings:\n",
    "            station_id = reading['station_id']\n",
    "            value = reading['value']\n",
    "            # Initialise the station_id key \n",
    "            if station_id not in data:\n",
    "                data[station_id] = {}\n",
    "            data[station_id][key] = value\n",
    "\n",
    "# Fetch station metadata\n",
    "response = requests.get(f\"{BASE_URL}air-temperature\")\n",
    "if response.status_code == 200:\n",
    "    stations = response.json()['metadata']['stations']\n",
    "    for station in stations:\n",
    "        station_id = station['id']\n",
    "        if station_id in data:\n",
    "            data[station_id]['latitude'] = station['location']['latitude']\n",
    "            data[station_id]['longitude'] = station['location']['longitude']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'station_id'}, inplace=True)\n",
    "\n",
    "# Add a timestamp column with the current time\n",
    "df['timestamp'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Reorder columns to match the desired format\n",
    "column_order = [\n",
    "    'timestamp', 'station_id', 'latitude', 'longitude', 'wind_direction',\n",
    "    'wind_speed', 'air_temperature', 'precipitation', 'relative_humidity'\n",
    "]\n",
    "df = df[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "api_data_path = 'weather_api_data.csv'\n",
    "df.to_csv(api_data_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {api_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad3554-dd50-4d97-a07f-882a8f9bd3d7",
   "metadata": {},
   "source": [
    "Unfortunately, as data only returns 3 different weather stations, we cannot map to the extent that we would like, so we build another function to generate new weather stations, where the data returned is based on distance between weather stations, It should be emphasised that this process is only for fullness of data and is not completely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982b35d-2a41-4bfc-ad36-d4f5f6481e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate weighted values\n",
    "def calculate_weighted_values(weights_matrix, original_values):\n",
    "    return np.around(weights_matrix.dot(original_values), decimals=1)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('weather_api_data.csv')\n",
    "\n",
    "# Initialise an empty DataFrame \n",
    "all_synthetic_data = pd.DataFrame()\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Process each group of data by unique timestamp\n",
    "for timestamp, group in df.groupby('timestamp'):\n",
    "    # Generate synthetic stations\n",
    "    num_synthetic_stations = 20\n",
    "\n",
    "    # Generate random latitudes and longitudes within the range of the group's data\n",
    "    lat_range = (group['latitude'].min(), group['latitude'].max())\n",
    "    lon_range = (group['longitude'].min(), group['longitude'].max())\n",
    "\n",
    "    synthetic_lat = np.random.uniform(lat_range[0], lat_range[1], num_synthetic_stations)\n",
    "    synthetic_lon = np.random.uniform(lon_range[0], lon_range[1], num_synthetic_stations)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute the distances from each synthetic station to the original stations\n",
    "    synthetic_coords = np.column_stack((synthetic_lat, synthetic_lon))\n",
    "    original_coords = group[['latitude', 'longitude']].values\n",
    "    distances = cdist(synthetic_coords, original_coords)\n",
    "\n",
    "    # Inverse distance weighting\n",
    "    weights = 1 / distances\n",
    "    normalized_weights = weights / weights.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calculate synthetic values \n",
    "    synthetic_wind_direction = np.around(calculate_weighted_values(normalized_weights, group['wind_direction'].values), decimals=1)\n",
    "    synthetic_wind_speed = np.around(calculate_weighted_values(normalized_weights, group['wind_speed'].values), decimals=1)\n",
    "    synthetic_temperature = np.around(calculate_weighted_values(normalized_weights, group['air_temperature'].values), decimals=1)\n",
    "    synthetic_precipitation = np.around(calculate_weighted_values(normalized_weights, group['precipitation'].values), decimals=1)\n",
    "    synthetic_humidity = np.around(calculate_weighted_values(normalized_weights, group['relative_humidity'].values), decimals=1)\n",
    "\n",
    "    \n",
    "    # Create station names\n",
    "    synthetic_station_names = [f'S{i+1}' for i in range(num_synthetic_stations)]\n",
    "\n",
    "    # Create a DataFrame for the synthetic data \n",
    "    synthetic_data = pd.DataFrame({\n",
    "        'timestamp': [timestamp] * num_synthetic_stations,\n",
    "        'station_id': synthetic_station_names,\n",
    "        'latitude': synthetic_lat,\n",
    "        'longitude': synthetic_lon,\n",
    "        'wind_direction': synthetic_wind_direction,\n",
    "        'wind_speed': synthetic_wind_speed,\n",
    "        'air_temperature': synthetic_temperature,\n",
    "        'precipitation': synthetic_precipitation,\n",
    "        'relative_humidity': synthetic_humidity\n",
    "    })\n",
    "\n",
    "    # Append the synthetic data to the all_synthetic_data DataFrame\n",
    "    all_synthetic_data = pd.concat([all_synthetic_data, synthetic_data], ignore_index=True)\n",
    "\n",
    "# Save the all synthetic data to a CSV file\n",
    "all_synthetic_data.to_csv('synth_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601efde3-538d-4907-884e-4bbd6e47a406",
   "metadata": {},
   "source": [
    "Furthermore, The pollutant data used throughout the system was manually gathered from \n",
    "https://www.haze.gov.sg/,\n",
    "\n",
    "while the api used for weather does also give access to these variables,problems were found in lining up the timestamps, and eventually manual insertion was the best way to go, pollution data was returned for the north, south, east, west and central areas of singapore, these were in the form of points, and not polygon or raster data, which was  problematic. while attempts to use a voronoi plot were made, these endeavours were unsuccessful, and manual insertion of the data was decidedly the best option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab41e7-1388-4f68-8e4f-5e7b7d75cef5",
   "metadata": {},
   "source": [
    "While the datasets are technically from different sources, they are still sourced from Singapore's open data collective, found at: https://beta.data.gov.sg/. since both datasets are accurate representations of their respective variables within the given timeframe, and are from a reliable source, the plotting of this data allows for a side-by-side representation of hourly rates of both pollution and weather, which is something that is hard to come by. And therefore analysis of this data could show fruitful results.\n",
    "\n",
    "It is also important to mention that while weather and pollution are what is mapped, other factors such as traffic or human activity could also be telling signs of why pollution rates may rise/fall\n",
    "\n",
    "some manual changes had to be made to the synthetic data gathered, this is where the discrepancy between synth_data.csv and synthetic_data.csv comes from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f3e2f-881f-424a-9f09-51cc169bb728",
   "metadata": {},
   "source": [
    "# design choices in the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654cf6a-591f-4082-9405-677edb17cc0b",
   "metadata": {},
   "source": [
    "Through researching interactive weather maps that display hourly weather change, the style of the maps tend to be rather minimalist, \n",
    "This is especially true when mapping rainfall, I found that online weather maps more often than not would only map rainfall. Fig 2  is an example of the type of maps I found When researching weather maps, I felt that for my means, this type of map was both too minimalist, and too complex, as there is too little data for a data scientist, and too much complex plotting (heatmaps) for casuals, therefore the use of choropleths, with scaling polygon colour schemes felt like the best way to go. On top of this, it must be admitted that the data would not allow for the building of decent heatmaps, largely due to the placement of stations. \n",
    "\n",
    "examples of designs I tried to avoid:\n",
    "https://www.bbc.co.uk/weather/map\n",
    "https://www.weatherandradar.co.uk/weather-map?center=54,-4.24&zoom=5.81\n",
    "\n",
    "I did not like the use of a satellite tileset due to my use of choropleth maps as it felt as if such colourful and animated maps have no place on such a realistic tileset. The default tileset in folium was much better as it felt more fitting for the data we were plotting, this is true for both the weather map and the pollution map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376d37-f4fd-40ce-bb07-57aa127415db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_example_weather_map(specific_timestamp, station_data, shapefile_path):\n",
    "    # Convert specific_timestamp from string to datetime object\n",
    "    specific_timestamp = pd.to_datetime(specific_timestamp, dayfirst=True)\n",
    "    \n",
    "    # Convert timestamps in station data and filter data\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "    hour = specific_timestamp.hour\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    \n",
    "    # Define the bounds of Singapore \n",
    "    lat_min, lat_max = 1.130, 1.450\n",
    "    lon_min, lon_max = 103.600, 104.100\n",
    "\n",
    "    # Number of data points\n",
    "    n_points = 1000\n",
    "\n",
    "    # Generate random latitude and longitude within Singapore's bounds\n",
    "    np.random.seed(42)  \n",
    "    latitudes = np.random.uniform(lat_min, lat_max, n_points)\n",
    "    longitudes = np.random.uniform(lon_min, lon_max, n_points)\n",
    "\n",
    "    # Generate synthetic rainfall data, where 0 is no rain and 1 is the highest intensity\n",
    "    rainfall_intensity = np.random.uniform(0, 1, n_points)\n",
    "\n",
    "    # Combine into a DataFrame\n",
    "    rainfall_data = pd.DataFrame({\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'intensity': rainfall_intensity\n",
    "    })\n",
    "    # Load and prepare Singapore shapefile\n",
    "    singapore_shapefile = gpd.read_file(f'{shapefile_path}/SGP_adm0.shp')\n",
    "    singapore_shapefile = singapore_shapefile.to_crs(gdf_stations.crs)\n",
    "    # Convert the rainfall DataFrame to a GeoDataFrame\n",
    "    gdf_rainfall = gpd.GeoDataFrame(rainfall_data, geometry=gpd.points_from_xy(rainfall_data.longitude, rainfall_data.latitude))\n",
    "\n",
    "    # Ensure the GeoDataFrame has the same CRS as the shapefile\n",
    "    gdf_rainfall.set_crs(singapore_shapefile.crs, inplace=True)\n",
    "\n",
    "    # Perform a spatial join to filter only the rainfall within Singapore\n",
    "    rainfall_within_singapore = gpd.sjoin(gdf_rainfall, singapore_shapefile, how=\"inner\", op='intersects')\n",
    "\n",
    "    # Extract the coordinates and intensity of rainfall points within Singapore\n",
    "    heatmap_data = rainfall_within_singapore[['latitude', 'longitude', 'intensity']].values.tolist()\n",
    "    \n",
    "    # Initialise the map \n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=11,\n",
    "               tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "               attr='Esri',\n",
    "               name='Esri Satellite')\n",
    "\n",
    "    # Loop through each row in the GeoDataFrame to place the text\n",
    "    for idx, row in gdf_stations.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            icon=DivIcon(\n",
    "                icon_size=(150,36),\n",
    "                icon_anchor=(7,20),\n",
    "                html=f'<div style=\"font-size: 16pt; color: white\">{row[\"air_temperature\"]}°C</div>',\n",
    "            )\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add heatmap layer for rainfall data\n",
    "    HeatMap(heatmap_data, min_opacity=0.2, max_val=float(rainfall_data['intensity'].max()), radius=15, blur=25, \n",
    "            gradient={0.2: 'blue', 0.4: 'cyan', 0.6: 'lime', 0.8: 'yellow', 1: 'red'}).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Usage\n",
    "station_data = pd.read_csv('synthetic_data.csv')\n",
    "map_object = generate_example_weather_map(\"23/04/2024 13:17\", station_data, 'shapefiles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cec812-4f6d-43e4-9c8b-88f18a1c7cae",
   "metadata": {},
   "source": [
    "# FIG 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e7f50-ced0-4a77-956f-434d4f0ee694",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10599f-18e2-4d37-9e3a-3346be30c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_coords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb68613-18e3-44b3-8573-ae036ceee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_humidity_map(specific_timestamp, station_data):\n",
    "    # Parse timestamps \n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "    # Filter data to include only entries from the same day as the specific timestamp\n",
    "    day_data = station_data[station_data['timestamp'].dt.date == specific_timestamp.date()]\n",
    "\n",
    "    # Ensure all timestamps are strings\n",
    "    day_data['timestamp'] = day_data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Define the full day's temperature range \n",
    "    global_min_temp = day_data['relative_humidity'].min()\n",
    "    global_max_temp = day_data['relative_humidity'].max()\n",
    "\n",
    "    # Filter data for the specific timestamp\n",
    "    specific_time_data = day_data[day_data['timestamp'] == specific_timestamp.strftime('%Y-%m-%d %H:%M:%S')]\n",
    "\n",
    "    if specific_time_data.empty:\n",
    "        raise ValueError(\"No data available for the given timestamp.\")\n",
    "\n",
    "    # Create GeoDataFrame for the specific timestamp stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "\n",
    "    #Stations shapefile is a manually created shapefile due to the failure of using a voronoi plot to define intra-region bounds\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    \n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Create a colormap with the full day's temperature range\n",
    "    colormap = LinearColormap(\n",
    "        ['deepskyblue', 'cyan','yellow', 'orange', 'red'],\n",
    "        vmin=global_min_temp,\n",
    "        vmax=global_max_temp,\n",
    "        caption='relative_humidity Scale'\n",
    "    )\n",
    "\n",
    "    # Initialise the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=10) \n",
    "\n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "    folium.GeoJson(\n",
    "        stations_clipped,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': colormap(feature['properties']['relative_humidity']) if 'relative_humidity' in feature['properties'] else 'transparent'\n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            fields=['relative_humidity'],\n",
    "            aliases=['Air relative_humidity:'],\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "                color: black;  /* Set text color to white */\n",
    "            \"\"\",\n",
    "            max_width=800\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    \n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d989ac0-8f32-4be0-bb8e-0ca5cdc236f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_temperature_map(specific_timestamp,station_data):\n",
    "    \n",
    "    # Load station data and parse timestamps\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "    \n",
    "    # Extract the hour from the specific_timestamp\n",
    "    hour = specific_timestamp.hour  # Extract hour from specific_timestamp\n",
    "    \n",
    "    # Filter data for hour\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    #convert to string to stop JSON serialisation error\n",
    "    gdf_stations['timestamp'] = gdf_stations['timestamp'].astype(str) \n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "    \n",
    "    #Stations shapefile is a manually created shapefile due to the failure of using a voronoi plot to define intra-region bounds\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    \n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, how=\"inner\",predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Convert clipped polygons to GeoJSON\n",
    "    stations_clipped_json = stations_clipped.to_json()\n",
    "\n",
    "    # Define global temperature range\n",
    "    global_min_temp = station_data['air_temperature'].min()\n",
    "    global_max_temp = station_data['air_temperature'].max()\n",
    "\n",
    "    # Create a step colormap with updated global range\n",
    "    step_colormap = StepColormap(\n",
    "        ['deepskyblue', 'cyan', 'lightskyblue', 'mediumseagreen', 'lightgreen', 'yellow', 'orange', 'red', 'darkred', 'maroon'],\n",
    "        vmin=global_min_temp,\n",
    "        vmax=global_max_temp,\n",
    "        index=[global_min_temp + i*(global_max_temp-global_min_temp)/9 for i in range(10)],\n",
    "        caption='Temperature Scale'\n",
    "    )\n",
    "\n",
    "    # Initialise the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=10)\n",
    "\n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "    folium.GeoJson(\n",
    "        stations_clipped_json,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': step_colormap(feature['properties']['air_temperature'])\n",
    "        },\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['air_temperature'],\n",
    "            aliases=[f\"Temperature at {specific_timestamp}PM:\"],\n",
    "            localize=True,\n",
    "            sticky=False,\n",
    "            labels=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "            \"\"\",\n",
    "            max_width=800),\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(step_colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707ba5-db04-43a9-872a-c4845dfa0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_precip_map(station_data,hour,rotation_angle,num_points):\n",
    "\n",
    "    def rotate_geometry(geom, angle, origin='centroid'):\n",
    "        if origin == 'centroid':\n",
    "             # Get the coordinates of the centroid\n",
    "            origin = geom.centroid.coords[0] \n",
    "        return rotate(geom, angle, origin=origin)\n",
    "\n",
    "\n",
    "    #manually defined rainfall LineStrings\n",
    "    df = pd.read_csv('rainfall_lines.csv')\n",
    "    df['linestring'] = df['linestring'].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='linestring')\n",
    "\n",
    "    #get time\n",
    "    current_time = datetime.now()  \n",
    "    features = []\n",
    "    \n",
    "    # Process each linestring\n",
    "    for index, row in gdf.iterrows():\n",
    "        original_line = row['linestring']\n",
    "        # rotate line strings to the angle of the wind directions\n",
    "        rotated_line = rotate_geometry(original_line, rotation_angle) \n",
    "\n",
    "        timespan = timedelta(seconds=10)\n",
    "        increment = timespan / num_points\n",
    "\n",
    "        for i in range(num_points + 1):\n",
    "            point = rotated_line.interpolate(rotated_line.length * i / num_points)\n",
    "            features.append({\n",
    "                'type': 'Feature',\n",
    "                'geometry': {\n",
    "                    'type': 'Point',\n",
    "                    'coordinates': [point.x, point.y]\n",
    "                },\n",
    "                'properties': {\n",
    "                    'time': (current_time + increment * i).isoformat(),\n",
    "                    'icon': 'circle',\n",
    "                    'iconSize': [2, 2],\n",
    "                    'fillColor': 'blue',\n",
    "                    'fillOpacity': 0.7,\n",
    "                    'stroke': 'false',\n",
    "                    'radius': 1\n",
    "                }\n",
    "            })\n",
    "\n",
    "    data = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "    \n",
    "    # Load station data and parse timestamps\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "\n",
    "    # Filter data for time\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    # Convert to string to prevent json serialisation errors\n",
    "    gdf_stations['timestamp'] = gdf_stations['timestamp'].astype(str) \n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, how=\"inner\", predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Convert clipped polygons to GeoJSON\n",
    "    stations_clipped_json = stations_clipped.to_json()\n",
    "\n",
    "    # Define global temperature range\n",
    "    global_min_precip = station_data['precipitation'].min()\n",
    "    global_max_precip = station_data['precipitation'].max()\n",
    "\n",
    "    # Create a step colormap with updated global range\n",
    "    index_points = [global_min_precip + i*(global_max_precip-global_min_precip)/4 for i in range(5)]\n",
    "\n",
    "    step_colormap = StepColormap(\n",
    "        ['lightblue', 'deepskyblue', 'dodgerblue', 'navy'],\n",
    "        vmin=global_min_precip,\n",
    "        vmax=global_max_precip,\n",
    "        index=index_points,  # Define the breakpoints for each color\n",
    "        caption='Rainfall Scale'\n",
    "    )\n",
    "    # Initialise the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=11)\n",
    "    def calculate_animation_speed(max_precip):\n",
    "    # Assume faster animations for higher precipitation\n",
    "        return max(1, 10 - max_precip / 10)  \n",
    "    \n",
    "    if station_data['precipitation'].any():\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "        \n",
    "    if show:\n",
    "        speed = calculate_animation_speed(global_max_precip)\n",
    "        TimestampedGeoJson(\n",
    "            data,\n",
    "            period='PT1S',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=True,\n",
    "            max_speed=speed,\n",
    "            date_options='YYYY/MM/DD HH:mm:ss',\n",
    "            transition_time=int(1000 / speed)\n",
    "        ).add_to(m)\n",
    "    \n",
    "    \n",
    "                   \n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "\n",
    "    folium.GeoJson(\n",
    "        stations_clipped_json,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': step_colormap(feature['properties']['precipitation'])\n",
    "        },\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['precipitation'],\n",
    "            aliases=[f\"rainfall mm at {hour} :\"],\n",
    "            localize=True,\n",
    "            sticky=False,\n",
    "            labels=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "            \"\"\",\n",
    "            max_width=800),\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(step_colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db1229-e197-488c-a95b-3691274938ec",
   "metadata": {},
   "source": [
    "While researching pollution maps, heatmaps were clearly the way to go, the colour gradient in heatmaps are great for representing hotspots, while also addressing that other areas are still also polluted to an extent. The pollution map is rather subtle and only changes slightly between the lower and upper bounds of the data, this is intentional as when using a larger gradient, some areas can have interest taken away from, and it is decidedly important that no area be ignored when considering Air pollution.\n",
    "\n",
    "The use of Inverse distance weighted interpolation took this map to the next level as basic heatmaps struggled to cover areas further away from the point the heatmarker was dropped at, and made these areas seem as if they were less effected, when in fact it was just a problem with the data.\n",
    "\n",
    "Inverse Distance Weighting (IDW) interpolation estimates unknown values using a weighted average of known values, emphasizing nearer over farther observations [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720dc3f-4805-46b5-baf5-244c3b28e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def idw_interpolation(x, y, z, grid_points, alpha=1.5):\n",
    "    # Convert pandas Series to NumPy arrays\n",
    "        x_array = np.array(x)\n",
    "        y_array = np.array(y)\n",
    "        z_array = np.array(z)\n",
    "    \n",
    "    # Calculate the distance matrix and raise to the power alpha\n",
    "        distances = cdist(grid_points, np.c_[x_array, y_array], 'euclidean')\n",
    "    \n",
    "    # Inverse distance weights\n",
    "        weights = 1 / np.power(distances, alpha)\n",
    "    \n",
    "    # Replace infinities with zeros to handle division by zero\n",
    "        weights[~np.isfinite(weights)] = 0\n",
    "    \n",
    "    # Calculate the weighted average\n",
    "        zi = np.dot(weights, z_array) / np.sum(weights, axis=1)\n",
    "    \n",
    "        return zi\n",
    "    \n",
    "def generate_pollution_heatmap(csv_file_path, shapefile_path, timestamp, pollutant, num_points=30):\n",
    "    # Load data and shapefiles\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    singapore_shapefile = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    #calculate percentiles for heatmap intensity scale\n",
    "    global_5th_percentile = df[pollutant].quantile(0.05)\n",
    "    global_95th_percentile = df[pollutant].quantile(0.95)\n",
    "    \n",
    "    # Filter Data for given timestamp\n",
    "    df_filtered = df[df['timestamp'] == timestamp]\n",
    "    df_filtered['geometry'] = df_filtered.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "    gdf_points = gpd.GeoDataFrame(df_filtered, geometry='geometry')\n",
    "    gdf_points.set_crs(singapore_shapefile.crs, inplace=True)\n",
    "    \n",
    "    # generate a grid of points over the area\n",
    "    x_min, y_min, x_max, y_max = singapore_shapefile.total_bounds\n",
    "    x_points = np.linspace(x_min, x_max, num_points)\n",
    "    y_points = np.linspace(y_min, y_max, num_points)\n",
    "    xx, yy = np.meshgrid(x_points, y_points)\n",
    "    grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    # Apply IDW interpolation to estimate pollution levels at grid points\n",
    "    z_values = idw_interpolation(gdf_points['longitude'], gdf_points['latitude'], gdf_points[pollutant], grid_points)\n",
    "    gdf_interpolated = gpd.GeoDataFrame({'geometry': gpd.points_from_xy(grid_points[:, 0], grid_points[:, 1]), 'value': z_values}, crs=singapore_shapefile.crs)\n",
    "    gdf_interpolated = gpd.sjoin(gdf_interpolated, singapore_shapefile, predicate='within')\n",
    "\n",
    "    # Prepare and plot Heatmap data with folium\n",
    "    heat_data = [[row.geometry.y, row.geometry.x, row.value] for idx, row in gdf_interpolated.iterrows() if np.isfinite(row.value)] \n",
    "    m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=11)\n",
    "    folium.plugins.HeatMap(heat_data, radius=15, blur=15, max_zoom=1, gradient={0: 'green', 0.5: 'yellow', 1: 'red'}, min_opacity=0.5, max_val=global_95th_percentile, min_val=global_5th_percentile).add_to(m)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388ba3f-c39c-4426-9cfd-666a1219c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and define plotting and data preparation functions\n",
    "def load_weather_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.split('.').str[0], format='%d/%m/%Y %H:%M')\n",
    "    return df\n",
    "\n",
    "def load_pollution_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['timestamp'], dayfirst=True)\n",
    "    return df \n",
    "\n",
    "df = load_weather_data('synthetic_data.csv')\n",
    "station_data = pd.read_csv('synthetic_data.csv')\n",
    "df_pollution = pd.read_csv('cleaned_air.csv') \n",
    "\n",
    "shapefile_path = 'shapefiles/SGP_adm0.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de8754-4555-4a3a-a04b-28415fec5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pollution data descriptors\n",
    "pollutant_descriptions = {\n",
    "    'PM10': 'PM10 (µg/m³)',\n",
    "    'PM25': 'PM2.5 (µg/m³)',\n",
    "    'O3': 'Ozone (µg/m³)',\n",
    "    'NO2': 'Nitrogen Dioxide (µg/m³)',\n",
    "    'CO': 'Carbon Monoxide (mg/m³)',\n",
    "    'SO2': 'Sulphur Dioxide (µg/m³)'\n",
    "}\n",
    "# Weather data discriptors\n",
    "weather_description = {\n",
    "    'wind_speed': 'Wind Speed (mph)',\n",
    "    'relative_humidity': 'relative humidity (RH)',\n",
    "    'precipitation': 'Precipitation (mm)',\n",
    "    'air_temperature': 'temperature (°C)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402150f-cc4a-460e-9b95-2ce639e08ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp slider\n",
    "timestamp_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Timestamp',\n",
    "    options=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).unique().tolist(),\n",
    "    value=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).iloc[0],\n",
    "    width=1000,\n",
    "    height= 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f254e5-9ae1-4076-aef7-af01f044de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Update temperature map depending on slider value\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_temperature_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    map_object = generate_temperature_map(specific_timestamp, station_data)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db4a48-7481-4075-87f4-7b3696a5b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update humidity map depending on slider value\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_humidity_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    map_object = generate_humidity_map(specific_timestamp, station_data)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551508f-576b-4863-aaee-1c5272944287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert degrees to radians\n",
    "def degrees_to_radians(deg):\n",
    "    return deg * np.pi / 180\n",
    "\n",
    "# Function to calculate the average wind direction\n",
    "def calculate_average_wind_direction(data):\n",
    "    sin_component = np.sin(degrees_to_radians(data['wind_direction']))\n",
    "    cos_component = np.cos(degrees_to_radians(data['wind_direction']))\n",
    "    \n",
    "    mean_sin = sin_component.mean()\n",
    "    mean_cos = cos_component.mean()\n",
    "    \n",
    "    avg_angle_rad = np.arctan2(mean_sin, mean_cos)\n",
    "    avg_angle_deg = np.degrees(avg_angle_rad) % 360\n",
    "    return avg_angle_deg\n",
    "\n",
    "average_wind_direction_hourly = station_data.groupby([station_data['timestamp'].dt.date, station_data['timestamp'].dt.hour]).apply(calculate_average_wind_direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92cb97-7d37-4eb9-aa98-45df4baf0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update precipitation map depending on slider value\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_precipitation_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    hour = specific_timestamp.hour\n",
    "    date = specific_timestamp.date()\n",
    "\n",
    "    # Fetch the average wind direction for the given date and hour\n",
    "    try:\n",
    "        rotation_angle = average_wind_direction_hourly.loc[(date, hour)]\n",
    "    except KeyError:\n",
    "        rotation_angle = 0  # Default to 0 if no data available\n",
    "\n",
    "    # Generate the map using the fetched wind direction\n",
    "    map_object = generate_precip_map(station_data, hour, rotation_angle=rotation_angle, numpoints = 15)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4237e4-82d4-459d-ad1f-320811abb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pollution map depending on slider value\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_pollution_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    print(specific_timestamp)\n",
    "    map_object = generate_pollution_heatmap(csv_file_path = 'cleaned_air.csv', shapefile_path ='shapefiles/SGP_adm0.shp', timestamp=timestamp, pollutant = 'SO2', num_points=30)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1737d99-36ec-49a1-a330-692bdf6f68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore  warning\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f67bf-ff63-4881-b692-430715ce6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of station Ids\n",
    "station_ids = list(df['station_id'].unique()) \n",
    "#define timestamp slider\n",
    "timestamp_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Timestamp',\n",
    "    options=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).unique().tolist(),\n",
    "    value=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).iloc[0],\n",
    "    width=1000,\n",
    "    height= 50\n",
    ")\n",
    "\n",
    "#Define weather widgets and corresponding values\n",
    "data_types = ['wind_speed', 'relative_humidity', 'precipitation', 'air_temperature']\n",
    "plotting_data_types = data_types\n",
    "data_type_toggle = pn.widgets.ToggleGroup(name='Data Type', options=data_types, behavior='radio')\n",
    "station_selector = pn.widgets.Select(name='Station ID', options=station_ids, width=200)\n",
    "map_type_selector = pn.widgets.Select(name='Choose Weather:', options=['Precipitation', 'Humidity', 'Temperature'], width=200)\n",
    "#map_region_selector = pn.widgets.Select(name='Map Region', options=['North', 'South', 'East', 'West', 'Central'], width=200)\n",
    "\n",
    "# Update weather data plotted on map depending on value in Select widget\n",
    "@pn.depends(timestamp=timestamp_slider.param.value, map_type=map_type_selector.param.value)\n",
    "def update_map_display(timestamp, map_type):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    \n",
    "    if map_type == 'Precipitation':\n",
    "        hour = specific_timestamp.hour\n",
    "        date = specific_timestamp.date()\n",
    "        # Fetch the average wind direction for the given date and hour\n",
    "        try:\n",
    "            rotation_angle = average_wind_direction_hourly.loc[(date, hour)]\n",
    "        except KeyError:\n",
    "            rotation_angle = 0  # Default to 0 if no data available\n",
    "        map_object = generate_precip_map(station_data, hour, rotation_angle=rotation_angle, num_points=15)\n",
    "    elif map_type == 'Humidity':\n",
    "        map_object = generate_humidity_map(specific_timestamp, station_data)\n",
    "    elif map_type == 'Temperature':\n",
    "        map_object = generate_temperature_map(specific_timestamp, station_data)\n",
    "    else:\n",
    "        return 'Select a valid map type'\n",
    "\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "#Define pollution widgets and corresponding values\n",
    "toggle_group = pn.widgets.ToggleGroup(name='Pollutant Toggle', options=['PM10', 'PM25', 'O3', 'NO2', 'CO'], behavior='radio')\n",
    "region_selector = pn.widgets.Select(name='Region', options=['North', 'South', 'East', 'West', 'Central'])\n",
    "pollutant_selector = pn.widgets.Select(name='Choose pollutant:', options=['PM10', 'PM25', 'O3', 'NO2', 'CO'], width=200)\n",
    "\n",
    "# Update pollution data plotted on map  depending on value selected in Select widget\n",
    "@pn.depends(timestamp=timestamp_slider.param.value, pollutant=pollutant_selector.param.value)\n",
    "def update_pollution_map(timestamp, pollutant):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    if pollutant is None:\n",
    "        return \"Please select a pollutant from the toggle above.\"\n",
    "\n",
    "    map_object = generate_pollution_heatmap(\n",
    "        csv_file_path='cleaned_air.csv',\n",
    "        shapefile_path='shapefiles/SGP_adm0.shp',\n",
    "        timestamp=timestamp,\n",
    "        pollutant=pollutant,\n",
    "        num_points=30\n",
    "    )\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "    \n",
    "# define pane to hold Select widgets\n",
    "forecast_pane = pn.Column(\n",
    "    pn.Column(map_type_selector,pollutant_selector)\n",
    ")\n",
    "\n",
    "# Spacers to align widgets\n",
    "spacer = pn.Spacer(width=450,height=100)  \n",
    "row_spacer = pn.Spacer(height=50)\n",
    "\n",
    "# widgets for pollution graph plot\n",
    "selector_column = pn.Column(\n",
    "    toggle_group,\n",
    "    region_selector\n",
    ")\n",
    "\n",
    "# Colour background\n",
    "full_row = pn.Row(\n",
    "    styles={'background': '#9fa2a6'}\n",
    ")\n",
    "\n",
    "# Maps row\n",
    "maps_row = pn.Row(  \n",
    "     #forecast_pane,update_map_display,update_pollution_map, styles={'background': '#9fa2a6'}\n",
    "     forecast_pane,update_map_display,update_pollution_map\n",
    ")\n",
    "\n",
    "# Slider row\n",
    "#slider_row = pn.Row(spacer,timestamp_slider, styles={'background': '#9fa2a6'})\n",
    "slider_row = pn.Row(spacer,timestamp_slider)\n",
    "\n",
    "# Full row\n",
    "forecast_map_layout = pn.Column(\n",
    "    maps_row,  # Maps\n",
    "    row_spacer,\n",
    "    slider_row,  # Slider\n",
    "    #styles={'background': '#9fa2a6'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eaf80-9ea5-4ecb-9444-fe056a2fb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combined pollution control and plot\n",
    "@pn.depends(pollutant=toggle_group.param.value, region=region_selector.param.value)\n",
    "def create_pollutant_plot(pollutant, region):\n",
    "    if not pollutant or not region:\n",
    "        return pn.pane.Markdown(\"Please select both a pollutant and a region.\")\n",
    "    filtered_data = df_pollution[df_pollution['region'] == region]\n",
    "    fig = px.line(filtered_data, x='timestamp', y=pollutant, title=f\"{pollutant_descriptions[pollutant]} Levels in {region} Singapore\",\n",
    "                  labels={'timestamp': 'Time', pollutant: f\"{pollutant_descriptions[pollutant]}\"},\n",
    "                  color_discrete_sequence=['#FFA500'])\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            # x ticklabels were full date string, rather than just year, cant fix better to remove\n",
    "            \n",
    "            tickmode='auto',  # Automatic tick placement\n",
    "            showticklabels=False,  # Do not show x-axis tick labels\n",
    "            title_text=''  # Remove x-axis title\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=pollutant_descriptions[pollutant]\n",
    "        ),\n",
    "        title=dict(text=f\"{pollutant_descriptions[pollutant]} Levels in {region}\", font=dict(size=10)),\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=pollutant_descriptions[pollutant],\n",
    "        width=500,\n",
    "        height=280,\n",
    "        plot_bgcolor='#dee3e0',\n",
    "        paper_bgcolor='#dee3e0',\n",
    "        font_color='black'  # Set font color to white \n",
    "    )\n",
    "    return fig\n",
    "\n",
    "# graph to represent weather data\n",
    "def plot_time_series(df, column, title, width=500, height=300):\n",
    "    fig = px.line(df, x='timestamp', y=column, title=title,\n",
    "                  labels={'timestamp': 'Time'},\n",
    "                  color_discrete_sequence=['#FFA500'])\n",
    "    fig.update_layout(\n",
    "        font=dict(size=8, color='black'),\n",
    "        title=dict(text=title, font=dict(size=10)),\n",
    "        plot_bgcolor='#dee3e0',\n",
    "        paper_bgcolor='#dee3e0',\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=weather_description.get(column, column),\n",
    "        width=width,\n",
    "        height=height\n",
    "    )\n",
    "    return fig    \n",
    "    \n",
    "# Pollution control and plot integrated in a single panel\n",
    "pollution_control_plot = pn.Column(\n",
    "    pn.Row(toggle_group, region_selector),\n",
    "    create_pollutant_plot\n",
    ")\n",
    "\n",
    "\n",
    "# Wind rose Plot \n",
    "def wind_news(df):\n",
    "    df = df.dropna(subset=['wind_direction', 'wind_speed'])\n",
    "    df.loc[:, 'wind_direction'] = df['wind_direction'] % 360  # Normalize direction to 0-360 degrees\n",
    "    \n",
    "    frames = []  # List to hold DataFrames to concatenate\n",
    "    angles = [(i * 22.5) for i in range(16)]\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    speeds = [i * 2.5 for i in range(1, 9)]\n",
    "    \n",
    "    for ang, dir_label in zip(angles, directions):\n",
    "        for s in speeds:\n",
    "            freq = df[(ang <= df['wind_direction']) & (df['wind_direction'] < ang + 22.5) &\n",
    "                      (s - 2.5 <= df['wind_speed']) & (df['wind_speed'] < s)].shape[0]\n",
    "            frames.append(pd.DataFrame({'direction': [dir_label], 'speed': [s], 'frequency': [freq]}))\n",
    "    \n",
    "    wind_df = pd.concat(frames, ignore_index=True)\n",
    "    return wind_df\n",
    "\n",
    "# grapth to represent wind direction through the wind rose plot\n",
    "def wind_direction_plot(df, title, width=300, height=340):\n",
    "    wind_df = wind_news(df)\n",
    "    fig = px.bar_polar(wind_df, r=\"frequency\", theta=\"direction\", color=\"speed\",\n",
    "                       template=\"plotly_dark\",\n",
    "                       color_discrete_sequence=px.colors.sequential.Plasma[-2::-1])\n",
    "    fig.update_layout(\n",
    "        title_text=f'Wind Direction',\n",
    "        width=width,\n",
    "        height=height,\n",
    "        plot_bgcolor='#dee3e0',\n",
    "        paper_bgcolor='#dee3e0',\n",
    "        font_color='black',\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20714c-0c3a-46a2-8edf-b8b8ac448aa2",
   "metadata": {},
   "source": [
    "# Design choices and interactivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06446fee-b38c-49be-ac00-157432c308ee",
   "metadata": {},
   "source": [
    "The final dashboard includes various interactivity choices, such as select boxes, sliders and togglegroup widgets, the idea is that the user should have all they need within the frame, or at least enough to analyse the data effectively, as previously stated, this dashboard is something of a proof-of-concept, and ideally would be deployed in a scenario that would have more than a days worth of data.\n",
    "\n",
    "The drop down boxes allow the user to choose the data that is plotted so that the user can explore for themselves, The time series slider\n",
    "performs similarly, allowing the user to go over the hours of the day to compare how the data has changed.\n",
    "\n",
    "The prcipitation map makes use of an animation that displays the direction of rainfall based on the wind direction, which is one of the more advanced features shown.\n",
    "\n",
    "outside of the map data, there are 3 plots on the bottom row of the dashboard, all allowing for a  more scientific analysis of the data plotted.\n",
    "\n",
    "outside of the widgets, there were two primary factors to consider, how the user understands the map on first glance. \n",
    "through testing individual values, setting zoom start to 11 was the best choice, as it encapsulates the full area that is used on the map,\n",
    "and tells the user what they need to know immediately, if initial zoom was any further away, then the precipitation animation becomes too clustered,\n",
    "and any closer, user would not be able to see the full map.\n",
    "\n",
    "Finally, the user of a colourbar is used to represent the scale of the data, so that a user may immediately understand the scope of where the current\n",
    "data is, in comparison to the min and max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858a76c-aad7-4ad1-a8c5-7e68795b5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update timeseries plot with Time slider\n",
    "@pn.depends(data_type=data_type_toggle.param.value)\n",
    "def update_time_series_plot(data_type):\n",
    "    avg_df = df.groupby('timestamp').agg({data_type: 'mean'}).reset_index()\n",
    "    title = f\"{data_type.capitalize()} Over Time\" \n",
    "    return plot_time_series(avg_df, data_type, title)\n",
    "\n",
    "# Update wind rose plot with time slider\n",
    "@pn.depends(data_type=data_type_toggle.param.value, timestamp=timestamp_slider.param.value)\n",
    "def update_wind_rose_plot(data_type, timestamp):\n",
    "    selected_time = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    filtered_df = df[df['timestamp'] == selected_time]\n",
    "    return wind_direction_plot(filtered_df, 'Average')\n",
    "    \n",
    "# Define weather graph\n",
    "weather_control_plot = pn.Column(\n",
    "    pn.Row(data_type_toggle),\n",
    "    update_time_series_plot\n",
    ")\n",
    "   \n",
    "# Bring all widgets into one place\n",
    "main_content = pn.Column(\n",
    "    forecast_map_layout,\n",
    "    #pn.Row(update_wind_rose_plot, weather_control_plot, pollution_control_plot, styles={'background': '#9fa2a6'}),\n",
    "    pn.Row(update_wind_rose_plot, weather_control_plot, pollution_control_plot,),\n",
    "    #styles={'background': '#9fa2a6'}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dashboard = pn.Row(\n",
    "    main_content,  \n",
    "    #styles={'background': '#9fa2a6'}\n",
    ")\n",
    "\n",
    "# Run dashboard\n",
    "dashboard.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96b33a-511e-4097-a0ac-070547552d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dashboard\n",
    "dashboard.save('dashboard.html', embed=True, resources=INLINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b67508-ef3a-4e79-b2b5-bd8d8066a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#run Dashboard html\n",
    "with open('dashboard.html', 'r',encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28755f61-95a0-4440-be9b-ac9f9036dab4",
   "metadata": {},
   "source": [
    "# relevant links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6c3f4-5005-4bbe-b409-fefa6580d431",
   "metadata": {},
   "source": [
    "Binder link:\n",
    "https://mybinder.org/v2/gh/Cdavison-development/testrepo/a876930751d62e3ffa7011377ca4a2420cbcab41?urlpath=lab%2Ftree%2FUntitled.ipynb\n",
    "\n",
    "Github:\n",
    "https://github.com/Cdavison-development/testrepo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aba254-777e-4fdb-97af-08c09fd8defc",
   "metadata": {},
   "source": [
    "# references\n",
    "\n",
    "[1]Liu Y, Zhou Y, Lu J. Exploring the relationship between air pollution and meteorological conditions in China under environmental governance. Sci Rep. 2020 Sep 3;10(1):14518. doi: 10.1038/s41598-020-71338-7. PMID: 32883992; PMCID: PMC7471117.\n",
    "\n",
    "[2]Dastoorpoor M, Idani E, Khanjani N, Goudarzi G, Bahrampour A. Relationship Between Air Pollution, Weather, Traffic, and Traffic-Related Mortality. Trauma Mon. 2016 Aug 22;21(4):e37585. doi: 10.5812/traumamon.37585. PMID: 28180125; PMCID: PMC5282930.\n",
    "\n",
    "[3]https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/spatial-autocorrelation.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404710e-ad6a-4f6e-93c1-2443c89bf6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
