{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import folium\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "from folium.features import GeoJsonTooltip\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "from branca.element import Element, Template, MacroElement\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "from shapely import wkt\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.affinity import rotate\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "import folium\n",
    "from folium.features import CustomIcon\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "import math\n",
    "from branca.colormap import linear\n",
    "\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00132d0-d0bc-4b58-97db-8a56c2bca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pn.extension('plotly', raw_css=[css])\n",
    "pn.extension('plotly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55623982-2f07-47d0-9273-f089e6109baa",
   "metadata": {},
   "source": [
    "# Dashboard Idea\n",
    "\n",
    "The overall idea of this dashboard was to visualise the pollution rates in comparison to the current weather and \n",
    "assess if any trend can be identified. As well as this, I wanted a platform where this data could be looked at somewhat casually, as one may \n",
    "view daily weather on their way to work. This is so that the map can be used by both data scientists, and the average person, giving it various usages to different parties. The idea  to use hourly data rather than daily or weekly data that may be more common for\n",
    "maps of similar nature was influenced by the premise previously described, as hourly data would be most valuable to the average user, and \n",
    "can still be beneficial for data science, especially considering that many weather/pollution analyses make use of data from a larger time period.\n",
    "Current popular studies such by Yansui et al [1] and Dastoorpoor et al [2], do not focus on hourly data, but rather daily and yearly, neither of \n",
    "which encapsulate what i want to show.\n",
    "\n",
    "Fig 1 is an example of a pollution / weather map sourced from the gov.uk website. Rather than being hourly data, it is bi-yearly data for all of the uk, comparing this map to the final map, which displays hourly data, it is apparent that this would only be used for data analysis means, which defeats the purpose of what I would like to create, however it does show some interesting trends when explored, I will leave the exploration up to the reader.\n",
    "\n",
    "\n",
    "Despite the use of hourly data, in theory we could have years worth of data in the dataset, which could tell even more of a story, however this\n",
    "amount of data was not available and is largely unfeasible\n",
    "\n",
    "I think it is an important thing to map as pollutant rates in contrast to weather is not discussed much, and improving accessibility to such data can be important as more people may grow more knowledgeable on the topic of pollution rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f0578-2cc6-45bc-97fe-f60fb8e3839b",
   "metadata": {},
   "source": [
    "# data\n",
    "we are using data from various sources, to gather weather data, we fetched data from a live API that returns : station coords, relative_humidity, air temperature,\n",
    "precipitation,wind speed and wind direction for the time of request. This data is gathered by 3 different station based around singapore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af71f98-9eb1-4d25-a8c7-977937e266a2",
   "metadata": {},
   "source": [
    "# Design choices in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61168-eeb0-4a66-88ea-048df1780a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "\n",
    "data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "\n",
    "# Load UK shapefile\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n",
    "\n",
    "def uk_weather_map(data,uk_shapefile,year, weather_param):\n",
    "    # Read in the pollution and climate dat\n",
    "\n",
    "    # Filter data for the specific year\n",
    "    specific_year_data = data[data['Year'] == year]\n",
    "\n",
    "    # Fetch the value for the weather parameter for the given year\n",
    "    if specific_year_data.empty:\n",
    "        raise ValueError(f\"No data available for the specified year for {pollution_param}\")\n",
    "    \n",
    "    value = specific_year_data[weather_param].iloc[0]\n",
    "\n",
    "    # Define the global range for the weather parameter\n",
    "    global_min = data[weather_param].min()\n",
    "    global_max = data[weather_param].max()\n",
    "\n",
    "    # Create a colormap\n",
    "    colormap = linear.YlOrRd_09.scale(global_min, global_max)\n",
    "    colormap.caption = f'avg {weather_param.capitalize()} Level for {year}'\n",
    "\n",
    "    # Initialize the map centered on the UK\n",
    "    m = folium.Map(location=[54.7023545, -3.2765753], zoom_start=6)\n",
    "\n",
    "    # Style function to color the entire shapefile based on the value\n",
    "    def style_function(feature):\n",
    "        return {\n",
    "            'fillColor': colormap(value),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.5\n",
    "        }\n",
    "\n",
    "    # Add the GeoJSON layer for the UK shapefile\n",
    "    folium.GeoJson(\n",
    "        uk_shapefile,\n",
    "        style_function=style_function,\n",
    "        tooltip=f' avg {weather_param.capitalize()}: {value}'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Examp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc28f6-7f36-46ef-a065-a491249fd1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "\n",
    "data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n",
    "\n",
    "def uk_pollution_map(data, shapefile, year, pollution_param):\n",
    "    # Filter data for the specific year\n",
    "    specific_year_data = data[data['Year'] == year]\n",
    "\n",
    "    # Fetch the value for the weather parameter for the given year\n",
    "    if specific_year_data.empty:\n",
    "        raise ValueError(f\"No data available for the specified year for {pollution_param}\")\n",
    "    \n",
    "    value = specific_year_data[pollution_param].iloc[0]\n",
    "\n",
    "    # Define the global range for the weather parameter\n",
    "    global_min = data[pollution_param].min()\n",
    "    global_max = data[pollution_param].max()\n",
    "\n",
    "    # Create a colormap\n",
    "    colormap = linear.YlOrRd_09.scale(global_min, global_max)\n",
    "    colormap.caption = f'Avg {pollution_param.capitalize()} Level for {year}'\n",
    "\n",
    "    # Initialize the map centered on the UK\n",
    "    m = folium.Map(location=[54.7023545, -3.2765753], zoom_start=6)\n",
    "\n",
    "    # Style function to color the entire shapefile based on the value\n",
    "    def style_function(feature):\n",
    "        return {\n",
    "            'fillColor': colormap(value),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.5\n",
    "        }\n",
    "\n",
    "    # Add the GeoJSON layer for the UK shapefile\n",
    "    folium.GeoJson(\n",
    "        shapefile,\n",
    "        style_function=style_function,\n",
    "        tooltip=f'Avg {pollution_param.capitalize()}: {value}'\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9fd59-3011-4aa3-b4ae-2d6f0926b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "\n",
    "# Load UK shapefile\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc8462-985b-475a-b959-cf5944f1b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp slider\n",
    "# Define the year slider\n",
    "\n",
    "uk_year_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Year',\n",
    "    options=[str(year) for year in [2015, 2017, 2019, 2021]],  # Generates a list of years from 2009 to 2021\n",
    "    value='2015',  # Default value as the first year in the range\n",
    "    width=1000,  # Adjust width as needed\n",
    "    height=30\n",
    ")\n",
    "\n",
    "uk_weather_selector = pn.widgets.Select(name='Choose weather:', options=['temperature','rainfall'], width=200)\n",
    "uk_pollutant_selector = pn.widgets.Select(name='Choose pollutant:', options=['PM10', 'PM25'], width=200)\n",
    "\n",
    "row_spacer = pn.Spacer(height=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35def31-fa8f-491d-9acb-ab4babcd1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution_data = pd.read_csv('uk_pollution_climate_data.csv')\n",
    "\n",
    "# Load UK shapefile\n",
    "uk_shapefile = gpd.read_file('shapefiles/GBR_adm0.shp')\n",
    "\n",
    "# Update function for weather map\n",
    "@pn.depends(year=uk_year_slider.param.value, weather_param=uk_weather_selector.param.value)\n",
    "def update_uk_weather_map(year, weather_param):\n",
    "    map_object = uk_weather_map(data, uk_shapefile, int(year), weather_param)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "# Update function for pollution map\n",
    "@pn.depends(year=uk_year_slider.param.value, pollution_param=uk_pollutant_selector.param.value)\n",
    "def update_uk_pollution_map(year, pollution_param):\n",
    "    map_object = uk_pollution_map(data, uk_shapefile, int(year), pollution_param)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827bbed-6c0e-409a-970c-a7f012d6a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine everything into a panel layout\n",
    "layout = pn.Column(\n",
    "    pn.Row(uk_weather_selector,uk_pollutant_selector),\n",
    "    pn.Row(update_uk_weather_map, update_uk_pollution_map),\n",
    "    pn.Row(row_spacer),\n",
    "    pn.Row(uk_year_slider)\n",
    ")\n",
    "\n",
    "# Serve the panel dashboard\n",
    "layout.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbb87d-cecd-4834-a46c-50768a8f313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.resources import INLINE\n",
    "layout.save('layout.html', embed=True, resources=INLINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914d292-4800-4992-bf42-1dd194780c87",
   "metadata": {},
   "source": [
    "# Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871558a2-ca08-4e55-81d0-c10cd1d42cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "with open('layout.html', 'r',encoding = 'utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ce2ea-05b6-4e8b-ab34-0ea5b9bbe384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Base URL for the API endpoints\n",
    "BASE_URL = \"https://api.data.gov.sg/v1/environment/\"\n",
    "\n",
    "# API endpoints\n",
    "endpoints = {\n",
    "    \"relative_humidity\": \"relative-humidity\",\n",
    "    \"air_temperature\": \"air-temperature\",\n",
    "    \"precipitation\": \"rainfall\",\n",
    "    \"wind_speed\": \"wind-speed\",\n",
    "    \"wind_direction\": \"wind-direction\"\n",
    "}\n",
    "\n",
    "# Fetch data from each endpoint\n",
    "data = {}\n",
    "for key, endpoint in endpoints.items():\n",
    "    response = requests.get(f\"{BASE_URL}{endpoint}\")\n",
    "    if response.status_code == 200:\n",
    "        # Assume we want the latest readings, hence index 0\n",
    "        readings = response.json()['items'][0]['readings']\n",
    "        for reading in readings:\n",
    "            station_id = reading['station_id']\n",
    "            value = reading['value']\n",
    "            # Initialize the station_id key if not already in the data dictionary\n",
    "            if station_id not in data:\n",
    "                data[station_id] = {}\n",
    "            data[station_id][key] = value\n",
    "\n",
    "# Fetch station metadata (assuming it's the same across all endpoints)\n",
    "response = requests.get(f\"{BASE_URL}air-temperature\")\n",
    "if response.status_code == 200:\n",
    "    stations = response.json()['metadata']['stations']\n",
    "    for station in stations:\n",
    "        station_id = station['id']\n",
    "        if station_id in data:\n",
    "            data[station_id]['latitude'] = station['location']['latitude']\n",
    "            data[station_id]['longitude'] = station['location']['longitude']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'station_id'}, inplace=True)\n",
    "\n",
    "# Add a timestamp column with the current time\n",
    "df['timestamp'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# Reorder columns to match the desired format\n",
    "column_order = [\n",
    "    'timestamp', 'station_id', 'latitude', 'longitude', 'wind_direction',\n",
    "    'wind_speed', 'air_temperature', 'precipitation', 'relative_humidity'\n",
    "]\n",
    "df = df[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "api_data_path = 'weather_api_data.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {api_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad3554-dd50-4d97-a07f-882a8f9bd3d7",
   "metadata": {},
   "source": [
    "Unfortunately, as data only returns 3 different weather stations, we cannot map to the extent that we would like, so we build another function to generate new weather stations, where the data returned is based on distance between weather stations, It should be emphasised that this process is only for fullness of data and is not completely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982b35d-2a41-4bfc-ad36-d4f5f6481e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Function to calculate weighted values\n",
    "def calculate_weighted_values(weights_matrix, original_values):\n",
    "    return np.around(weights_matrix.dot(original_values), decimals=1)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('weather_api_data.csv') # Replace with the correct path\n",
    "\n",
    "# Initialize an empty DataFrame to hold all synthetic data\n",
    "all_synthetic_data = pd.DataFrame()\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Process each group of data by unique timestamp\n",
    "for timestamp, group in df.groupby('timestamp'):\n",
    "    # Generate synthetic stations\n",
    "    num_synthetic_stations = 20\n",
    "\n",
    "    # Generate random latitudes and longitudes within the range of the group's data\n",
    "    lat_range = (group['latitude'].min(), group['latitude'].max())\n",
    "    lon_range = (group['longitude'].min(), group['longitude'].max())\n",
    "\n",
    "    synthetic_lat = np.random.uniform(lat_range[0], lat_range[1], num_synthetic_stations)\n",
    "    synthetic_lon = np.random.uniform(lon_range[0], lon_range[1], num_synthetic_stations)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute the distances from each synthetic station to the original stations\n",
    "    synthetic_coords = np.column_stack((synthetic_lat, synthetic_lon))\n",
    "    original_coords = group[['latitude', 'longitude']].values\n",
    "    distances = cdist(synthetic_coords, original_coords)\n",
    "\n",
    "    # Inverse distance weighting\n",
    "    weights = 1 / distances\n",
    "    normalized_weights = weights / weights.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calculate synthetic values for all the required fields\n",
    "# Calculate synthetic values for all the required fields\n",
    "    synthetic_wind_direction = np.around(calculate_weighted_values(normalized_weights, group['wind_direction'].values), decimals=1)\n",
    "    synthetic_wind_speed = np.around(calculate_weighted_values(normalized_weights, group['wind_speed'].values), decimals=1)\n",
    "    synthetic_temperature = np.around(calculate_weighted_values(normalized_weights, group['air_temperature'].values), decimals=1)\n",
    "    synthetic_precipitation = np.around(calculate_weighted_values(normalized_weights, group['precipitation'].values), decimals=1)\n",
    "    synthetic_humidity = np.around(calculate_weighted_values(normalized_weights, group['relative_humidity'].values), decimals=1)\n",
    "\n",
    "    \n",
    "    # Create station names\n",
    "    synthetic_station_names = [f'S{i+1}' for i in range(num_synthetic_stations)]\n",
    "\n",
    "    # Create a DataFrame for the synthetic data including all fields and the timestamp\n",
    "    synthetic_data = pd.DataFrame({\n",
    "        'timestamp': [timestamp] * num_synthetic_stations,\n",
    "        'station_id': synthetic_station_names,\n",
    "        'latitude': synthetic_lat,\n",
    "        'longitude': synthetic_lon,\n",
    "        'wind_direction': synthetic_wind_direction,\n",
    "        'wind_speed': synthetic_wind_speed,\n",
    "        'air_temperature': synthetic_temperature,\n",
    "        'precipitation': synthetic_precipitation,\n",
    "        'relative_humidity': synthetic_humidity\n",
    "    })\n",
    "\n",
    "    # Append the synthetic data to the all_synthetic_data DataFrame\n",
    "    all_synthetic_data = pd.concat([all_synthetic_data, synthetic_data], ignore_index=True)\n",
    "\n",
    "# Save the all synthetic data to a CSV file\n",
    "all_synthetic_data.to_csv('syntheti_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601efde3-538d-4907-884e-4bbd6e47a406",
   "metadata": {},
   "source": [
    "Furthermore, The pollutant data used throughout the system was manually gathered from \n",
    "https://www.haze.gov.sg/,\n",
    "\n",
    "while the api used for weather does also give access to these variables,problems were found in lining up the timestamps, and eventually manual insertion was the best way to go, pollution data was returned for the north, south, east, west and central areas of singapore, these were in the form of points, and not polygon or raster data, which was  problematic. while attempts to use a voronoi plot were made, these endeavours were unsuccessful, and manual insertion of the data was decidedly the best option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab41e7-1388-4f68-8e4f-5e7b7d75cef5",
   "metadata": {},
   "source": [
    "While the datasets are technically from different sources, they are still sourced from Singapore's open data collective, found at: https://beta.data.gov.sg/. since both datasets are accurate representations of their respective variables within the given timeframe, and are from a reliable source, the plotting of this data allows for a side-by-side representation of hourly rates of both pollution and weather, which is something that is hard to come by. And therefore analysis of this data could show fruitful results.\n",
    "\n",
    "It is also important to mentoin that while weather and pollution are what is mapped, other factors such as traffic or human activity could also be telling signs of why pollution rates may rise/fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cfd20-d09d-47c1-9dce-25df6e033bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discuss singapore and their temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec3bb0-112a-46a0-a9c7-3bf94f9e0a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babee8b8-b0e1-4a37-b06d-c09a2b6280ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discuss singapore and the common pollutants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f03f6-9be7-4d44-b0d3-e895cc044346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link the topics together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f3e2f-881f-424a-9f09-51cc169bb728",
   "metadata": {},
   "source": [
    "# design choices in the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654cf6a-591f-4082-9405-677edb17cc0b",
   "metadata": {},
   "source": [
    "Through researching interactive weather maps that display hourly weather change, the style of the maps tend to be rather minimalist, \n",
    "This is especially true when mapping rainfall, I found that online weather maps more often than not would only map rainfall. Fig 2  is an example of the type of maps I found When researching weather maps, I felt as if for my means, this type of map was both too minimalist, and too complex, as there is too little data for a data scientist, and too much complex plotting (heatmaps) for casuals, therefore the use of choropleths, with scaling polygon colour schemes felt like the best way to go. On top of this, it must be admitted that the data would not allow for the building of decent heatmaps, largely due to the placement of stations. \n",
    "\n",
    "examples of designs I tried to avoid:\n",
    "https://www.bbc.co.uk/weather/map\n",
    "https://www.weatherandradar.co.uk/weather-map?center=54,-4.24&zoom=5.81\n",
    "\n",
    "I personally did not like the use of a satellite tileset considering the use of choropleth maps as it felt as if such colourful and animated maps have no place on such a realistic tileset. The default tileset in folium was much better as it felt more fitting for the data we were plotting, this is true for both the weather map and the pollution map\n",
    "\n",
    "On top of this, it must be admitted that the weather data would not allow for the building of decent heatmaps, largely due to the placement of stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376d37-f4fd-40ce-bb07-57aa127415db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import DivIcon\n",
    "\n",
    "def generate_example_weather_map(specific_timestamp, station_data, shapefile_path):\n",
    "    # Convert specific_timestamp from string to datetime object\n",
    "    specific_timestamp = pd.to_datetime(specific_timestamp, dayfirst=True)\n",
    "    \n",
    "    # Convert timestamps in station data and filter data\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "    hour = specific_timestamp.hour\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    # Define the bounds of Singapore (approximate)\n",
    "    lat_min, lat_max = 1.130, 1.450\n",
    "    lon_min, lon_max = 103.600, 104.100\n",
    "\n",
    "    # Number of data points\n",
    "    n_points = 1000\n",
    "\n",
    "# Generate random latitude and longitude within Singapore's bounds\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    latitudes = np.random.uniform(lat_min, lat_max, n_points)\n",
    "    longitudes = np.random.uniform(lon_min, lon_max, n_points)\n",
    "\n",
    "# Generate synthetic rainfall data, where 0 is no rain and 1 is the highest intensity\n",
    "    rainfall_intensity = np.random.uniform(0, 1, n_points)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "    rainfall_data = pd.DataFrame({\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'intensity': rainfall_intensity\n",
    "    })\n",
    "    # Load and prepare Singapore shapefile\n",
    "    singapore_shapefile = gpd.read_file(f'{shapefile_path}/SGP_adm0.shp')\n",
    "    singapore_shapefile = singapore_shapefile.to_crs(gdf_stations.crs)\n",
    "# Convert the rainfall DataFrame to a GeoDataFrame\n",
    "    gdf_rainfall = gpd.GeoDataFrame(rainfall_data, geometry=gpd.points_from_xy(rainfall_data.longitude, rainfall_data.latitude))\n",
    "\n",
    "    # Ensure the GeoDataFrame has the same CRS as the shapefile\n",
    "    gdf_rainfall.set_crs(singapore_shapefile.crs, inplace=True)\n",
    "\n",
    "    # Perform a spatial join to filter only the rainfall within Singapore\n",
    "    rainfall_within_singapore = gpd.sjoin(gdf_rainfall, singapore_shapefile, how=\"inner\", op='intersects')\n",
    "\n",
    "    # Extract the coordinates and intensity of rainfall points within Singapore\n",
    "    heatmap_data = rainfall_within_singapore[['latitude', 'longitude', 'intensity']].values.tolist()\n",
    "    \n",
    "    # Initialize the map centered on Singapore with the Esri Satellite tileset\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=11,\n",
    "               tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "               attr='Esri',\n",
    "               name='Esri Satellite')\n",
    "\n",
    "    # Loop through each row in the GeoDataFrame to place the text\n",
    "    for idx, row in gdf_stations.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            icon=DivIcon(\n",
    "                icon_size=(150,36),\n",
    "                icon_anchor=(7,20),\n",
    "                html=f'<div style=\"font-size: 16pt; color: white\">{row[\"air_temperature\"]}Â°C</div>',\n",
    "            )\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add heatmap layer for rainfall data\n",
    "    HeatMap(heatmap_data, min_opacity=0.2, max_val=float(rainfall_data['intensity'].max()), radius=15, blur=25, \n",
    "            gradient={0.2: 'blue', 0.4: 'cyan', 0.6: 'lime', 0.8: 'yellow', 1: 'red'}).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Usage\n",
    "station_data = pd.read_csv('synthetic_data.csv')\n",
    "map_object = generate_example_weather_map(\"23/04/2024 13:17\", station_data, 'shapefiles')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cec812-4f6d-43e4-9c8b-88f18a1c7cae",
   "metadata": {},
   "source": [
    "# FIG 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e7f50-ced0-4a77-956f-434d4f0ee694",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10599f-18e2-4d37-9e3a-3346be30c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_coords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb68613-18e3-44b3-8573-ae036ceee58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import LinearColormap\n",
    "from folium.features import GeoJsonTooltip\n",
    "\n",
    "def generate_humidity_map(specific_timestamp, station_data):\n",
    "    # Parse timestamps correctly\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "    # Filter data to include only entries from the same day as the specific timestamp\n",
    "    day_data = station_data[station_data['timestamp'].dt.date == specific_timestamp.date()]\n",
    "\n",
    "    # Ensure all timestamps are strings for serialization\n",
    "    day_data['timestamp'] = day_data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Define the full day's temperature range for coloring\n",
    "    global_min_temp = day_data['relative_humidity'].min()\n",
    "    global_max_temp = day_data['relative_humidity'].max()\n",
    "\n",
    "    # Filter data for the specific timestamp\n",
    "    specific_time_data = day_data[day_data['timestamp'] == specific_timestamp.strftime('%Y-%m-%d %H:%M:%S')]\n",
    "\n",
    "    if specific_time_data.empty:\n",
    "        raise ValueError(\"No data available for the given timestamp.\")\n",
    "\n",
    "    # Create GeoDataFrame for the specific timestamp stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Create a colormap with the full day's temperature range\n",
    "    colormap = LinearColormap(\n",
    "        ['deepskyblue', 'cyan','yellow', 'orange', 'red'],\n",
    "        vmin=global_min_temp,\n",
    "        vmax=global_max_temp,\n",
    "        caption='relative_humidity Scale'\n",
    "    )\n",
    "\n",
    "    # Initialize the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=10)  # Proper attribution)\n",
    "\n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "    folium.GeoJson(\n",
    "        stations_clipped,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': colormap(feature['properties']['relative_humidity']) if 'relative_humidity' in feature['properties'] else 'transparent'\n",
    "        },\n",
    "        tooltip=GeoJsonTooltip(\n",
    "            fields=['relative_humidity'],\n",
    "            aliases=['Air relative_humidity:'],\n",
    "            localize=True,\n",
    "            sticky=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "                color: black;  /* Set text color to white */\n",
    "            \"\"\",\n",
    "            max_width=800\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    \n",
    "    # Add the colormap to the map\n",
    "    m.add_child(colormap)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d989ac0-8f32-4be0-bb8e-0ca5cdc236f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "from branca.element import Element, Template, MacroElement\n",
    "\n",
    "def generate_temperature_map(specific_timestamp,station_data):\n",
    "    # Load station data and parse timestamps\n",
    "    \n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "    # Extract the hour from the specific_timestamp\n",
    "    hour = specific_timestamp.hour  # Extract hour from specific_timestamp\n",
    "    # Filter data for measurements at 3 PM (15:00 hours)\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    gdf_stations['timestamp'] = gdf_stations['timestamp'].astype(str)  # Convert to string to avoid serialization issues\n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, how=\"inner\",predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Convert clipped polygons to GeoJSON\n",
    "    stations_clipped_json = stations_clipped.to_json()\n",
    "\n",
    "    # Define global temperature range\n",
    "    global_min_temp = station_data['air_temperature'].min()\n",
    "    global_max_temp = station_data['air_temperature'].max()\n",
    "\n",
    "    # Create a step colormap with updated global range\n",
    "    step_colormap = StepColormap(\n",
    "        ['deepskyblue', 'cyan', 'lightskyblue', 'mediumseagreen', 'lightgreen', 'yellow', 'orange', 'red', 'darkred', 'maroon'],\n",
    "        vmin=global_min_temp,\n",
    "        vmax=global_max_temp,\n",
    "        index=[global_min_temp + i*(global_max_temp-global_min_temp)/9 for i in range(10)],\n",
    "        caption='Temperature Scale'\n",
    "    )\n",
    "\n",
    "    # Initialize the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=10)\n",
    "\n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "    folium.GeoJson(\n",
    "        stations_clipped_json,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': step_colormap(feature['properties']['air_temperature'])\n",
    "        },\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['air_temperature'],\n",
    "            aliases=[f\"Temperature at {specific_timestamp}PM:\"],\n",
    "            localize=True,\n",
    "            sticky=False,\n",
    "            labels=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "            \"\"\",\n",
    "            max_width=800),\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(step_colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707ba5-db04-43a9-872a-c4845dfa0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from branca.colormap import StepColormap\n",
    "from shapely import wkt\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.affinity import rotate\n",
    "def generate_precip_map(station_data,hour,rotation_angle,num_points):\n",
    "\n",
    "    def rotate_geometry(geom, angle, origin='centroid'):\n",
    "        if origin == 'centroid':\n",
    "            origin = geom.centroid.coords[0]  # Get the coordinates of the centroid\n",
    "        return rotate(geom, angle, origin=origin)\n",
    "\n",
    "    \n",
    "    df = pd.read_csv('rainfall_lines.csv')\n",
    "    df['linestring'] = df['linestring'].apply(wkt.loads)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='linestring')\n",
    "\n",
    "    current_time = datetime.now()  # degrees\n",
    "\n",
    "    features = []\n",
    "    # Process each linestring\n",
    "    for index, row in gdf.iterrows():\n",
    "        original_line = row['linestring']\n",
    "        # Simulate rotate_geometry function from previous usage\n",
    "        rotated_line = rotate_geometry(original_line, rotation_angle)   # This should be replaced by actual rotation logic if needed\n",
    "\n",
    "        timespan = timedelta(seconds=10)\n",
    "        increment = timespan / num_points\n",
    "\n",
    "        for i in range(num_points + 1):\n",
    "            point = rotated_line.interpolate(rotated_line.length * i / num_points)\n",
    "            features.append({\n",
    "                'type': 'Feature',\n",
    "                'geometry': {\n",
    "                    'type': 'Point',\n",
    "                    'coordinates': [point.x, point.y]\n",
    "                },\n",
    "                'properties': {\n",
    "                    'time': (current_time + increment * i).isoformat(),\n",
    "                    'icon': 'circle',\n",
    "                    'iconSize': [2, 2],\n",
    "                    'fillColor': 'blue',\n",
    "                    'fillOpacity': 0.7,\n",
    "                    'stroke': 'false',\n",
    "                    'radius': 1\n",
    "                }\n",
    "            })\n",
    "\n",
    "    data = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': features\n",
    "    }\n",
    "    # Load station data and parse timestamps\n",
    "    \n",
    "    \n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "\n",
    "    # Filter data for measurements at 3 PM (15:00 hours)\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    gdf_stations['timestamp'] = gdf_stations['timestamp'].astype(str)  # Convert to string to avoid serialization issues\n",
    "\n",
    "    # Load and prepare shapefiles\n",
    "    stations_shapefile = gpd.read_file('shapefiles/output_shapefile.shp')\n",
    "    singapore_shapefile = gpd.read_file('shapefiles/SGP_adm0.shp')\n",
    "    stations_shapefile = stations_shapefile.to_crs(gdf_stations.crs)\n",
    "\n",
    "    # Join station data with shapefile\n",
    "    stations_with_data = gpd.sjoin(stations_shapefile, gdf_stations, how=\"inner\", predicate='contains')\n",
    "    stations_clipped = gpd.clip(stations_with_data, singapore_shapefile)\n",
    "\n",
    "    # Convert clipped polygons to GeoJSON\n",
    "    stations_clipped_json = stations_clipped.to_json()\n",
    "\n",
    "    # Define global temperature range\n",
    "    global_min_temp = station_data['precipitation'].min()\n",
    "    global_max_temp = station_data['precipitation'].max()\n",
    "\n",
    "    # Create a step colormap with updated global range\n",
    "    index_points = [global_min_temp + i*(global_max_temp-global_min_temp)/4 for i in range(5)]\n",
    "\n",
    "    step_colormap = StepColormap(\n",
    "        ['lightblue', 'deepskyblue', 'dodgerblue', 'navy'],\n",
    "        vmin=global_min_temp,\n",
    "        vmax=global_max_temp,\n",
    "        index=index_points,  # Define the breakpoints for each color\n",
    "        caption='Rainfall Scale'\n",
    "    )\n",
    "    # Initialize the map\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=11)\n",
    "    def calculate_animation_speed(max_precip):\n",
    "    # Assume faster animations for higher precipitation\n",
    "        return max(1, 10 - max_precip / 10)  # Example formula, adjust as needed\n",
    "    \n",
    "    if station_data['precipitation'].any():\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "        \n",
    "    if show:\n",
    "        speed = calculate_animation_speed(global_max_temp)\n",
    "        TimestampedGeoJson(\n",
    "            data,\n",
    "            period='PT1S',\n",
    "            add_last_point=True,\n",
    "            auto_play=True,\n",
    "            loop=True,\n",
    "            max_speed=speed,\n",
    "            date_options='YYYY/MM/DD HH:mm:ss',\n",
    "            transition_time=int(1000 / speed)\n",
    "        ).add_to(m)\n",
    "    \n",
    "    \n",
    "                   \n",
    "    # Add the GeoJSON layer for clipped polygons\n",
    "\n",
    "    folium.GeoJson(\n",
    "        stations_clipped_json,\n",
    "        style_function=lambda feature: {\n",
    "            'fillOpacity': 0.5,\n",
    "            'weight': 2,\n",
    "            'color': 'black',\n",
    "            'fillColor': step_colormap(feature['properties']['precipitation'])\n",
    "        },\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['precipitation'],\n",
    "            aliases=[f\"rainfall mm at {hour} :\"],\n",
    "            localize=True,\n",
    "            sticky=False,\n",
    "            labels=True,\n",
    "            style=\"\"\"\n",
    "                background-color: #F0EFEF;\n",
    "                border: 2px solid black;\n",
    "                border-radius: 3px;\n",
    "                box-shadow: 3px;\n",
    "            \"\"\",\n",
    "            max_width=800),\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the colormap to the map\n",
    "    m.add_child(step_colormap)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db1229-e197-488c-a95b-3691274938ec",
   "metadata": {},
   "source": [
    "While researching pollution maps, heatmaps were clearly the way to go, the colour gradient in heatmaps are great for representing hotspots, while also addressing that other areas are still also polluted to an extent. The way that the pollution map is rather subtle and only changes slightly between the lower and upper bounds of the data is intentional as when using a larger gradient, some areas can have interest taken away from, and it is decidedly important that no area be ignored when considering Air pollution.\n",
    "\n",
    "The use of Inverse distance weighted interpolation took this map to the next level as basic heatmaps struggled to cover areas further away from the point the heatmarker was dropped at, and made these areas seem as if they were less effected, when in fact it was just a problem with the data.\n",
    "\n",
    "Inverse Distance Weighting (IDW) interpolation estimates unknown values using a weighted average of known values, emphasizing nearer over farther observations [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720dc3f-4805-46b5-baf5-244c3b28e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely.geometry import Point\n",
    "def idw_interpolation(x, y, z, grid_points, alpha=1.5):\n",
    "    # Convert pandas Series to NumPy arrays if not already\n",
    "        x_array = np.array(x)\n",
    "        y_array = np.array(y)\n",
    "        z_array = np.array(z)\n",
    "    \n",
    "    # Calculate the distance matrix and raise to the power alpha\n",
    "        distances = cdist(grid_points, np.c_[x_array, y_array], 'euclidean')\n",
    "    \n",
    "    # Inverse distance weights\n",
    "        weights = 1 / np.power(distances, alpha)\n",
    "    \n",
    "    # Replace infinities with zeros to handle division by zero\n",
    "        weights[~np.isfinite(weights)] = 0\n",
    "    \n",
    "    # Calculate the weighted average\n",
    "        zi = np.dot(weights, z_array) / np.sum(weights, axis=1)\n",
    "    \n",
    "        return zi\n",
    "    \n",
    "def generate_pollution_heatmap(csv_file_path, shapefile_path, timestamp, pollutant, num_points=30):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    singapore_shapefile = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    global_5th_percentile = df[pollutant].quantile(0.05)\n",
    "    global_95th_percentile = df[pollutant].quantile(0.95)\n",
    "\n",
    "    df_filtered = df[df['timestamp'] == timestamp]\n",
    "    df_filtered['geometry'] = df_filtered.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "    gdf_points = gpd.GeoDataFrame(df_filtered, geometry='geometry')\n",
    "    gdf_points.set_crs(singapore_shapefile.crs, inplace=True)\n",
    "\n",
    "    x_min, y_min, x_max, y_max = singapore_shapefile.total_bounds\n",
    "    x_points = np.linspace(x_min, x_max, num_points)\n",
    "    y_points = np.linspace(y_min, y_max, num_points)\n",
    "    xx, yy = np.meshgrid(x_points, y_points)\n",
    "    grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    \n",
    "    z_values = idw_interpolation(gdf_points['longitude'], gdf_points['latitude'], gdf_points[pollutant], grid_points)\n",
    "    gdf_interpolated = gpd.GeoDataFrame({'geometry': gpd.points_from_xy(grid_points[:, 0], grid_points[:, 1]), 'value': z_values}, crs=singapore_shapefile.crs)\n",
    "    gdf_interpolated = gpd.sjoin(gdf_interpolated, singapore_shapefile, predicate='within')\n",
    "    heat_data = [[row.geometry.y, row.geometry.x, row.value] for idx, row in gdf_interpolated.iterrows() if np.isfinite(row.value)]\n",
    "    \n",
    "    m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=11)\n",
    "    folium.plugins.HeatMap(heat_data, radius=15, blur=15, max_zoom=1, gradient={0: 'green', 0.5: 'yellow', 1: 'red'}, min_opacity=0.5, max_val=global_95th_percentile, min_val=global_5th_percentile).add_to(m)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac310a-e460-4188-ab13-df87f5590fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folium\n",
    "from folium.features import CustomIcon\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388ba3f-c39c-4426-9cfd-666a1219c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and define plotting and data preparation functions\n",
    "# Function to load weather data\n",
    "def load_weather_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str.split('.').str[0], format='%d/%m/%Y %H:%M')\n",
    "    return df\n",
    "\n",
    "def load_pollution_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['timestamp'], dayfirst=True)\n",
    "    return df # Convert map to HTML\n",
    "\n",
    "df = load_weather_data('synthetic_data2.csv')\n",
    "pollution_data = load_pollution_data('cleaned_air2.csv')\n",
    "\n",
    "df_weather = pd.read_csv('synthetic_data2.csv')  # Update with the correct path and necessary preprocessing\n",
    "df_pollution = pd.read_csv('cleaned_air2.csv') \n",
    "\n",
    "shapefile_path = 'shapefiles/SGP_adm0.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de8754-4555-4a3a-a04b-28415fec5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pollution data descriptors\n",
    "pollutant_descriptions = {\n",
    "    'PM10': 'PM10 (Âµg/mÂ³)',\n",
    "    'PM25': 'PM2.5 (Âµg/mÂ³)',\n",
    "    'O3': 'Ozone (Âµg/mÂ³)',\n",
    "    'NO2': 'Nitrogen Dioxide (Âµg/mÂ³)',\n",
    "    'CO': 'Carbon Monoxide (mg/mÂ³)',\n",
    "    'SO2': 'Sulphur Dioxide (Âµg/mÂ³)'\n",
    "}\n",
    "\n",
    "weather_description = {\n",
    "    'wind_speed': 'Wind Speed (mph)',\n",
    "    'relative_humidity': 'relative humidity (RH)',\n",
    "    'precipitation': 'Precipitation (mm)',\n",
    "    'air_temperature': 'temperature (Â°C)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402150f-cc4a-460e-9b95-2ce639e08ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the timestamp slider\n",
    "timestamp_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Timestamp',\n",
    "    options=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).unique().tolist(),\n",
    "    value=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).iloc[0],\n",
    "    width=1000,\n",
    "    height= 50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f254e5-9ae1-4076-aef7-af01f044de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates and updates the folium map\n",
    "def create_folium_map(timestamp_str, df):\n",
    "    specific_timestamp = pd.to_datetime(timestamp_str, format='%d/%m/%Y %H:%M')\n",
    "    specific_data = df[df['timestamp'] == specific_timestamp]\n",
    "\n",
    "    # Initialize map\n",
    "    m = folium.Map(location=[specific_data['latitude'].mean(), specific_data['longitude'].mean()], zoom_start=5)\n",
    "\n",
    "    # Add markers\n",
    "    for idx, row in specific_data.iterrows():\n",
    "        folium.Marker(\n",
    "            [row['latitude'], row['longitude']],\n",
    "            popup=f'<i>Temp: {row[\"air_temperature\"]} C</i>',\n",
    "            icon=folium.Icon(color=\"blue\")\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "    \n",
    "station_data = pd.read_csv('synthetic_data2.csv')\n",
    "station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "# Define a reactive function to update the map based on the slider\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_temperature_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    map_object = generate_temperature_map(specific_timestamp, station_data)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db4a48-7481-4075-87f4-7b3696a5b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_humidity_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    map_object = generate_humidity_map(specific_timestamp, station_data)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551508f-576b-4863-aaee-1c5272944287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert degrees to radians\n",
    "\n",
    "def degrees_to_radians(deg):\n",
    "    return deg * np.pi / 180\n",
    "\n",
    "# Function to calculate the average wind direction\n",
    "def calculate_average_wind_direction(data):\n",
    "    sin_component = np.sin(degrees_to_radians(data['wind_direction']))\n",
    "    cos_component = np.cos(degrees_to_radians(data['wind_direction']))\n",
    "    \n",
    "    mean_sin = sin_component.mean()\n",
    "    mean_cos = cos_component.mean()\n",
    "    \n",
    "    avg_angle_rad = np.arctan2(mean_sin, mean_cos)\n",
    "    avg_angle_deg = np.degrees(avg_angle_rad) % 360\n",
    "    return avg_angle_deg\n",
    "\n",
    "average_wind_direction_hourly = station_data.groupby([station_data['timestamp'].dt.date, station_data['timestamp'].dt.hour]).apply(calculate_average_wind_direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92cb97-7d37-4eb9-aa98-45df4baf0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a reactive function to update the map based on the slider\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_precipitation_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    hour = specific_timestamp.hour\n",
    "    date = specific_timestamp.date()\n",
    "\n",
    "    # Fetch the average wind direction for the given date and hour\n",
    "    try:\n",
    "        rotation_angle = average_wind_direction_hourly.loc[(date, hour)]\n",
    "    except KeyError:\n",
    "        rotation_angle = 0  # Default to 0 if no data available\n",
    "\n",
    "    # Generate the map using the fetched wind direction\n",
    "    map_object = generate_precip_map(station_data, hour, rotation_angle=rotation_angle, numpoints = 15)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4237e4-82d4-459d-ad1f-320811abb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pn.depends(timestamp=timestamp_slider.param.value)\n",
    "def update_pollution_map(timestamp):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    print(specific_timestamp)\n",
    "    map_object = generate_pollution_heatmap(csv_file_path = 'cleaned_air2.csv', shapefile_path ='shapefiles/SGP_adm0.shp', timestamp=timestamp, pollutant = 'SO2', num_points=30)\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1737d99-36ec-49a1-a330-692bdf6f68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings (not recommended)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ignore specific warnings by category (better practice)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f67bf-ff63-4881-b692-430715ce6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = list(df['station_id'].unique()) \n",
    "\n",
    "timestamp_slider = pn.widgets.DiscreteSlider(\n",
    "    name='Timestamp',\n",
    "    options=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).unique().tolist(),\n",
    "    value=pd.Series(df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')).iloc[0],\n",
    "    width=1000,\n",
    "    height= 50\n",
    ")\n",
    "\n",
    "data_types = ['wind_speed', 'relative_humidity', 'precipitation', 'air_temperature']\n",
    "plotting_data_types = data_types\n",
    "#data_type_selector = pn.widgets.Select(name='Data Type', options=data_types, width=200)\n",
    "data_type_toggle = pn.widgets.ToggleGroup(name='Data Type', options=data_types, behavior='radio')\n",
    "station_selector = pn.widgets.Select(name='Station ID', options=station_ids, width=200)\n",
    "\n",
    "map_type_selector = pn.widgets.Select(name='Choose Weather:', options=['Precipitation', 'Humidity', 'Temperature'], width=200)\n",
    "map_region_selector = pn.widgets.Select(name='Map Region', options=['North', 'South', 'East', 'West', 'Central'], width=200)\n",
    "\n",
    "# Add custom CSS to target the 'custom-slider' class if you want to apply additional custom styling\n",
    "@pn.depends(timestamp=timestamp_slider.param.value, map_type=map_type_selector.param.value)\n",
    "def update_map_display(timestamp, map_type):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    \n",
    "    if map_type == 'Precipitation':\n",
    "        hour = specific_timestamp.hour\n",
    "        date = specific_timestamp.date()\n",
    "        # Fetch the average wind direction for the given date and hour\n",
    "        try:\n",
    "            rotation_angle = average_wind_direction_hourly.loc[(date, hour)]\n",
    "        except KeyError:\n",
    "            rotation_angle = 0  # Default to 0 if no data available\n",
    "        map_object = generate_precip_map(station_data, hour, rotation_angle=rotation_angle, num_points=15)\n",
    "    elif map_type == 'Humidity':\n",
    "        map_object = generate_humidity_map(specific_timestamp, station_data)\n",
    "    elif map_type == 'Temperature':\n",
    "        map_object = generate_temperature_map(specific_timestamp, station_data)\n",
    "    else:\n",
    "        return 'Select a valid map type'\n",
    "\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "# Control columns\n",
    "\n",
    "# Toggle and region selection\n",
    "toggle_group = pn.widgets.ToggleGroup(name='Pollutant Toggle', options=['PM10', 'PM25', 'O3', 'NO2', 'CO'], behavior='radio')\n",
    "region_selector = pn.widgets.Select(name='Region', options=['North', 'South', 'East', 'West', 'Central'])\n",
    "pollutant_selector = pn.widgets.Select(name='Choose pollutant:', options=['PM10', 'PM25', 'O3', 'NO2', 'CO'], width=200)\n",
    "\n",
    "@pn.depends(timestamp=timestamp_slider.param.value, pollutant=pollutant_selector.param.value)\n",
    "def update_pollution_map(timestamp, pollutant):\n",
    "    specific_timestamp = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    if pollutant is None:\n",
    "        return \"Please select a pollutant from the toggle above.\"\n",
    "\n",
    "    map_object = generate_pollution_heatmap(\n",
    "        csv_file_path='cleaned_air2.csv',\n",
    "        shapefile_path='shapefiles/SGP_adm0.shp',\n",
    "        timestamp=timestamp,\n",
    "        pollutant=pollutant,\n",
    "        num_points=30\n",
    "    )\n",
    "    return pn.pane.HTML(map_object._repr_html_(), width=600, height=320)\n",
    "\n",
    "forecast_pane = pn.Column(\n",
    "    pn.Column(map_type_selector,pollutant_selector)\n",
    ")\n",
    "spacer = pn.Spacer(width=450,height=100)  # Adjust the width as needed\n",
    "row_spacer = pn.Spacer(height=50)\n",
    "selector_column = pn.Column(\n",
    "    toggle_group,\n",
    "    region_selector\n",
    ")\n",
    "# Full row combining controls and maps\n",
    "full_row = pn.Row(\n",
    "    styles={'background': 'black'}\n",
    ")\n",
    "# Maps row\n",
    "maps_row = pn.Row(  \n",
    "     forecast_pane,update_map_display,update_pollution_map, styles={'background': 'black'}\n",
    ")\n",
    "\n",
    "# Slider row\n",
    "#slider_row = pn.Row(controls_column,spacer,timestamp_slider, background='black')\n",
    "slider_row = pn.Row(spacer,timestamp_slider, styles={'background': 'black'})\n",
    "\n",
    "forecast_map_layout = pn.Column(\n",
    "    #full_row,  # Controls\n",
    "    maps_row,  # Maps\n",
    "    slider_row,  # Slider\n",
    "    #row_spacer,\n",
    "    styles={'background': '#black'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695eaf80-9ea5-4ecb-9444-fe056a2fb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Custom function to select every fourth hour\n",
    "def custom_tick_values(start, end, freq):\n",
    "    all_ticks = pd.date_range(start=start, end=end, freq=freq)\n",
    "    selected_ticks = all_ticks[::4]  # Select every fourth value\n",
    "    return selected_ticks\n",
    "# Assume your timestamps are on a specific day and you want ticks every 2 hours\n",
    "tick_vals = pd.date_range(start='2024-04-22 00:00', periods=12, freq='2H')\n",
    "tick_labels = [t.strftime('%I %p').lstrip('0') for t in tick_vals]\n",
    "# Combined pollution control and plot\n",
    "@pn.depends(pollutant=toggle_group.param.value, region=region_selector.param.value)\n",
    "def create_pollutant_plot(pollutant, region):\n",
    "    if not pollutant or not region:\n",
    "        return pn.pane.Markdown(\"Please select both a pollutant and a region.\")\n",
    "    filtered_data = df_pollution[df_pollution['region'] == region]\n",
    "    fig = px.line(filtered_data, x='timestamp', y=pollutant, title=f\"{pollutant_descriptions[pollutant]} Levels in {region} Singapore\",\n",
    "                  labels={'timestamp': 'Time', pollutant: f\"{pollutant_descriptions[pollutant]}\"},\n",
    "                  color_discrete_sequence=['#FFA500'])\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickmode='auto',  # Automatic tick placement\n",
    "            showticklabels=False,  # Do not show x-axis tick labels\n",
    "            title_text=''  # Remove x-axis title\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=pollutant_descriptions[pollutant]\n",
    "        ),\n",
    "        title=dict(text=f\"{pollutant_descriptions[pollutant]} Levels in {region}\", font=dict(size=10)),\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=pollutant_descriptions[pollutant],\n",
    "        width=500,\n",
    "        height=300,\n",
    "        plot_bgcolor='#000000',\n",
    "        paper_bgcolor='#000000',\n",
    "        font_color='white'  # Set font color to white for all textual elements\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def plot_time_series(df, column, title, width=500, height=300):\n",
    "    fig = px.line(df, x='timestamp', y=column, title=title,\n",
    "                  labels={'timestamp': 'Time'},\n",
    "                  color_discrete_sequence=['#FFA500'])\n",
    "    fig.update_layout(\n",
    "        font=dict(size=8, color='white'),\n",
    "        title=dict(text=title, font=dict(size=10)),\n",
    "        plot_bgcolor='#000000',\n",
    "        paper_bgcolor='#000000',\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=weather_description.get(column, column),\n",
    "        width=width,\n",
    "        height=height\n",
    "    )\n",
    "    return fig    \n",
    "# Pollution control and plot integrated in a single panel\n",
    "pollution_control_plot = pn.Column(\n",
    "    pn.Row(toggle_group, region_selector),\n",
    "    create_pollutant_plot\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def wind_news(df):\n",
    "    df = df.dropna(subset=['wind_direction', 'wind_speed'])\n",
    "    df.loc[:, 'wind_direction'] = df['wind_direction'] % 360  # Normalize direction to 0-360 degrees\n",
    "    \n",
    "    frames = []  # List to hold DataFrames to concatenate\n",
    "    angles = [(i * 22.5) for i in range(16)]\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    speeds = [i * 2.5 for i in range(1, 9)]\n",
    "    \n",
    "    for ang, dir_label in zip(angles, directions):\n",
    "        for s in speeds:\n",
    "            freq = df[(ang <= df['wind_direction']) & (df['wind_direction'] < ang + 22.5) &\n",
    "                      (s - 2.5 <= df['wind_speed']) & (df['wind_speed'] < s)].shape[0]\n",
    "            frames.append(pd.DataFrame({'direction': [dir_label], 'speed': [s], 'frequency': [freq]}))\n",
    "    \n",
    "    wind_df = pd.concat(frames, ignore_index=True)\n",
    "    return wind_df\n",
    "\n",
    "def wind_direction_plot(df, title, width=300, height=300):\n",
    "    wind_df = wind_news(df)\n",
    "    fig = px.bar_polar(wind_df, r=\"frequency\", theta=\"direction\", color=\"speed\",\n",
    "                       template=\"plotly_dark\",\n",
    "                       color_discrete_sequence=px.colors.sequential.Plasma[-2::-1])\n",
    "    fig.update_layout(\n",
    "        title_text=f'Wind Direction',\n",
    "        width=width,\n",
    "        height=height,\n",
    "        plot_bgcolor='#000000',\n",
    "        paper_bgcolor='#333030',\n",
    "        font_color='white',\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20714c-0c3a-46a2-8edf-b8b8ac448aa2",
   "metadata": {},
   "source": [
    "# Design choices and interactivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858a76c-aad7-4ad1-a8c5-7e68795b5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pn.depends(data_type=data_type_toggle.param.value)\n",
    "def update_time_series_plot(data_type):\n",
    "    avg_df = df.groupby('timestamp').agg({data_type: 'mean'}).reset_index()\n",
    "    title = f\"{data_type.capitalize()} Over Time\"  # Creating a title based on data type\n",
    "    return plot_time_series(avg_df, data_type, title)\n",
    "\n",
    "@pn.depends(data_type=data_type_toggle.param.value, timestamp=timestamp_slider.param.value)\n",
    "def update_wind_rose_plot(data_type, timestamp):\n",
    "    selected_time = pd.to_datetime(timestamp, format='%d/%m/%Y %H:%M')\n",
    "    filtered_df = df[df['timestamp'] == selected_time]\n",
    "    return wind_direction_plot(filtered_df, 'Average')\n",
    "# You may include the toggle group directly above or within the plot panel\n",
    "weather_control_plot = pn.Column(\n",
    "    pn.Row(data_type_toggle),\n",
    "    update_time_series_plot\n",
    ")\n",
    "   \n",
    "# Define the main content that includes all your current dashboard elements\n",
    "# Incorporate the new control into the main content\n",
    "main_content = pn.Column(\n",
    "    forecast_map_layout,\n",
    "    pn.Row(update_wind_rose_plot, weather_control_plot, pollution_control_plot, styles={'background': 'black'}),\n",
    "    styles={'background': 'black'}\n",
    ")\n",
    "\n",
    "\n",
    "# Final layout with the new column on the far right, spanning all rows\n",
    "dashboard = pn.Row(\n",
    "    main_content,  # This will take the majority of the space # This new column will be on the far right\n",
    "    styles={'background': 'black'}\n",
    ")\n",
    "\n",
    "\n",
    "dashboard.servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96b33a-511e-4097-a0ac-070547552d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.resources import INLINE\n",
    "dashboard.save('dashboard.html', embed=True, resources=INLINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b67508-ef3a-4e79-b2b5-bd8d8066a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Assuming 'example.html' is your HTML file\n",
    "with open('dashboard.html', 'r',encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Display the HTML content in the notebook\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0ade8-302a-4138-ac0d-87350fee1981",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a41b51d-c7ee-4a5e-8a0b-1335d1ab0332",
   "metadata": {},
   "source": [
    "### fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6829c5-53af-4a45-a4d0-845be75315e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.features import DivIcon\n",
    "\n",
    "def generate_example_weather_map(specific_timestamp, station_data, shapefile_path):\n",
    "    # Convert specific_timestamp from string to datetime object\n",
    "    specific_timestamp = pd.to_datetime(specific_timestamp, dayfirst=True)\n",
    "    \n",
    "    # Convert timestamps in station data and filter data\n",
    "    station_data['timestamp'] = pd.to_datetime(station_data['timestamp'], dayfirst=True)\n",
    "    hour = specific_timestamp.hour\n",
    "    specific_time_data = station_data[station_data['timestamp'].dt.hour == hour]\n",
    "\n",
    "    # Create GeoDataFrame for stations\n",
    "    gdf_stations = gpd.GeoDataFrame(\n",
    "        specific_time_data,\n",
    "        geometry=gpd.points_from_xy(specific_time_data.longitude, specific_time_data.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    # Define the bounds of Singapore (approximate)\n",
    "    lat_min, lat_max = 1.130, 1.450\n",
    "    lon_min, lon_max = 103.600, 104.100\n",
    "\n",
    "    # Number of data points\n",
    "    n_points = 1000\n",
    "\n",
    "# Generate random latitude and longitude within Singapore's bounds\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    latitudes = np.random.uniform(lat_min, lat_max, n_points)\n",
    "    longitudes = np.random.uniform(lon_min, lon_max, n_points)\n",
    "\n",
    "# Generate synthetic rainfall data, where 0 is no rain and 1 is the highest intensity\n",
    "    rainfall_intensity = np.random.uniform(0, 1, n_points)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "    rainfall_data = pd.DataFrame({\n",
    "        'latitude': latitudes,\n",
    "        'longitude': longitudes,\n",
    "        'intensity': rainfall_intensity\n",
    "    })\n",
    "    # Load and prepare Singapore shapefile\n",
    "    singapore_shapefile = gpd.read_file(f'{shapefile_path}/SGP_adm0.shp')\n",
    "    singapore_shapefile = singapore_shapefile.to_crs(gdf_stations.crs)\n",
    "# Convert the rainfall DataFrame to a GeoDataFrame\n",
    "    gdf_rainfall = gpd.GeoDataFrame(rainfall_data, geometry=gpd.points_from_xy(rainfall_data.longitude, rainfall_data.latitude))\n",
    "\n",
    "    # Ensure the GeoDataFrame has the same CRS as the shapefile\n",
    "    gdf_rainfall.set_crs(singapore_shapefile.crs, inplace=True)\n",
    "\n",
    "    # Perform a spatial join to filter only the rainfall within Singapore\n",
    "    rainfall_within_singapore = gpd.sjoin(gdf_rainfall, singapore_shapefile, how=\"inner\", op='intersects')\n",
    "\n",
    "    # Extract the coordinates and intensity of rainfall points within Singapore\n",
    "    heatmap_data = rainfall_within_singapore[['latitude', 'longitude', 'intensity']].values.tolist()\n",
    "    \n",
    "    # Initialize the map centered on Singapore with the Esri Satellite tileset\n",
    "    m = folium.Map(location=[1.3521, 103.8198], zoom_start=11,\n",
    "               tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "               attr='Esri',\n",
    "               name='Esri Satellite')\n",
    "\n",
    "    # Loop through each row in the GeoDataFrame to place the text\n",
    "    for idx, row in gdf_stations.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            icon=DivIcon(\n",
    "                icon_size=(150,36),\n",
    "                icon_anchor=(7,20),\n",
    "                html=f'<div style=\"font-size: 16pt; color: white\">{row[\"air_temperature\"]}Â°C</div>',\n",
    "            )\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add heatmap layer for rainfall data\n",
    "    HeatMap(heatmap_data, min_opacity=0.2, max_val=float(rainfall_data['intensity'].max()), radius=15, blur=25, \n",
    "            gradient={0.2: 'blue', 0.4: 'cyan', 0.6: 'lime', 0.8: 'yellow', 1: 'red'}).add_to(m)\n",
    "\n",
    "    return m\n",
    "\n",
    "# Usage\n",
    "station_data = pd.read_csv('synthetic_data.csv')\n",
    "map_object = generate_example_weather_map(\"23/04/2024 13:17\", station_data, 'shapefiles')\n",
    "map_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aba254-777e-4fdb-97af-08c09fd8defc",
   "metadata": {},
   "source": [
    "# references\n",
    "\n",
    "[1]Liu Y, Zhou Y, Lu J. Exploring the relationship between air pollution and meteorological conditions in China under environmental governance. Sci Rep. 2020 Sep 3;10(1):14518. doi: 10.1038/s41598-020-71338-7. PMID: 32883992; PMCID: PMC7471117.\n",
    "\n",
    "[2]Dastoorpoor M, Idani E, Khanjani N, Goudarzi G, Bahrampour A. Relationship Between Air Pollution, Weather, Traffic, and Traffic-Related Mortality. Trauma Mon. 2016 Aug 22;21(4):e37585. doi: 10.5812/traumamon.37585. PMID: 28180125; PMCID: PMC5282930.\n",
    "\n",
    "[3]https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/spatial-autocorrelation.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404710e-ad6a-4f6e-93c1-2443c89bf6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
